{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import plotter as pltr\n",
    "pltr.set_backend(pltr.MatplotlibBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as dutils\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep and Visualizing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [0.5]\n",
    "stds = [0.5]\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "white = pltr.Color(red=255, green=255, blue=255)\n",
    "big_white_font = pltr.Font(color=white, size=14)\n",
    "\n",
    "Metric = namedtuple('Metric', ['name', 'train', 'val'])\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    \"\"\"\n",
    "    Tensor of size 1xHxW: Supposed to be an image with a single channel\n",
    "    Returns an ndarray of shape HxW of type int with values between 0 and 255.\n",
    "    \"\"\"\n",
    "    img = copy(tensor.cpu().detach().numpy())\n",
    "    img = img.squeeze()  # Get rid fo the single channel dim\n",
    "    img = img * stds + means  # De-normalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = 255 * img  # Re-scale to 0-255\n",
    "    img = img.astype(np.int)\n",
    "    return img\n",
    "\n",
    "def show_imgs(imgs, titles):\n",
    "    \"\"\"\n",
    "    imgs: list of ndarrays of shape HxW\n",
    "    titles: list of strings\n",
    "    \"\"\"\n",
    "    frame = pltr.Frame()\n",
    "    frame.layout(1, len(imgs))  # Show all the images in a single row\n",
    "    for img, title in zip(imgs, titles):\n",
    "        chart = frame.create_chart()\n",
    "        chart.title = title\n",
    "        chart.title_font = big_white_font\n",
    "        chart.show_axes = False\n",
    "        imgplot = pltr.GrayscaleImage(img)\n",
    "        chart.add(imgplot)\n",
    "    frame.show()\n",
    "    \n",
    "def conv_dims(n, k, s=1, p=0):\n",
    "    return (n + 2*p - k)/s + 1\n",
    "\n",
    "def deconv_dims(n, k, s=1, p=0):\n",
    "    return (n-1)*s + k - 2*p\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "trainset = dsets.MNIST('/data/pytorch/mnist', download=True, train=True, transform=xforms)\n",
    "trainloader = dutils.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "images, labels = next(iter(trainloader))\n",
    "imgs = [tensor2img(images[0]), tensor2img(images[1])]\n",
    "titles = [str(labels[0].item()), str(labels[1].item())]\n",
    "show_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network\n",
    "\n",
    "#### Layer 1\n",
    "1x28x28 -- CONV(3) --> RELU --> 64x26x26\n",
    "\n",
    "#### Layer 2\n",
    "64x?x? -- CONV(3) --> BNORM --> RELU --> 128x24x24\n",
    "\n",
    "#### Layer 3\n",
    "128x24x24 -- CONV(4, 2, 1) --> BNORM --> RELU --> 256x12x12\n",
    "\n",
    "#### Layer 4\n",
    "256x12x12 -- CONV(4, 2, 1) --> BNORM --> RELU --> 512x6x6\n",
    "\n",
    "#### Layer 5\n",
    "512x6x6 -- CONV(6, 1) --> 1x1x1 -- SIGMOID --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (relu1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (conv5): Conv2d(512, 1, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, bias=False)),\n",
    "    ('relu1', nn.LeakyReLU(0.2, inplace=True)),\n",
    "    \n",
    "    ('conv2', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, bias=False)),\n",
    "    ('bn2', nn.BatchNorm2d(128)),\n",
    "    ('relu2', nn.LeakyReLU(0.2, inplace=True)),\n",
    "    \n",
    "    ('conv3', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "    ('bn3', nn.BatchNorm2d(256)),\n",
    "    ('relu3', nn.LeakyReLU(0.2, inplace=True)),\n",
    "    \n",
    "    ('conv4', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "    ('bn4', nn.BatchNorm2d(512)),\n",
    "    ('relu4', nn.LeakyReLU(0.2, inplace=True)),\n",
    "    \n",
    "    ('conv5', nn.Conv2d(in_channels=512, out_channels=1, kernel_size=6, bias=False)),\n",
    "    ('sigmoid', nn.Sigmoid())\n",
    "]))\n",
    "\n",
    "discriminator.to(DEVICE)\n",
    "discriminator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "\n",
    "The input noise vector will be z_dims in size. When it goes into the deconv layer, think of it as a 1x1 image with z_dims channels going in. Let z_dims be 100.\n",
    "\n",
    "#### Layer 1\n",
    "100x1x1 -- DECONV(6) --> BNORM --> RELU --> 512x6x6\n",
    "\n",
    "#### Layer 2\n",
    "512x6x6 -- DECONV(4, 2, 1) --> BNORM --> RELU --> 256x12x12\n",
    "\n",
    "#### Layer 3\n",
    "256x12x12 -- DECONV(4, 2, 1) --> BNORM --> RELU --> 128x24x24\n",
    "\n",
    "#### Layer 4\n",
    "128x24x24 -- DECONV(3) --> BNORM --> RELU --> 64x26x26\n",
    "\n",
    "#### Layer 5\n",
    "64x26x26 -- DECONV(3) --> TANH --> 1x28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv1): ConvTranspose2d(100, 512, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace)\n",
       "  (deconv2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace)\n",
       "  (deconv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU(inplace)\n",
       "  (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU(inplace)\n",
       "  (deconv5): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = nn.Sequential(OrderedDict([\n",
    "    ('deconv1', nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=6, bias=False)),\n",
    "    ('bn1', nn.BatchNorm2d(512)),\n",
    "    ('relu1', nn.ReLU(512)),\n",
    "    \n",
    "    ('deconv2', nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "    ('bn2', nn.BatchNorm2d(256)),\n",
    "    ('relu2', nn.ReLU(True)),\n",
    "    \n",
    "    ('deconv3', nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "    ('bn3', nn.BatchNorm2d(128)),\n",
    "    ('relu3', nn.ReLU(True)),\n",
    "    \n",
    "    ('deconv4', nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, bias=False)),\n",
    "    ('bn4', nn.BatchNorm2d(64)),\n",
    "    ('relu4', nn.ReLU(True)),\n",
    "    \n",
    "    ('deconv5', nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=3, bias=False)),\n",
    "    ('tanh', nn.Tanh())\n",
    "]))\n",
    "\n",
    "generator.to(DEVICE)\n",
    "generator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "optimD = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimG = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "........................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f0d248a2fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mhvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlossD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mlossD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    epoch_lossD = 0\n",
    "    epoch_lossG = 0\n",
    "    \n",
    "    for images, _ in trainloader:\n",
    "        print('.', end='')\n",
    "        with torch.enable_grad():\n",
    "            optimD.zero_grad()\n",
    "            optimG.zero_grad()\n",
    "\n",
    "            real_images = images.to(DEVICE)\n",
    "\n",
    "            noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=DEVICE)\n",
    "            fake_images = generator.forward(noise)\n",
    "\n",
    "            # Train discriminator - on real images\n",
    "            labels = torch.full((BATCH_SIZE,), real_label, device=DEVICE)\n",
    "            hvals = discriminator.forward(real_images).squeeze()\n",
    "            lossD_real = loss_fn(hvals, labels)\n",
    "            lossD_real.backward()\n",
    "\n",
    "            # Train discriminator - on fake images\n",
    "            labels.fill_(fake_label)\n",
    "            hvals = discriminator.forward(fake_images.detach()).squeeze()\n",
    "            lossD_fake = loss_fn(hvals, labels)\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            # Update the discriminator weights\n",
    "            optimD.step()\n",
    "\n",
    "            # Train generator on fake images labeled as real\n",
    "            labels.fill_(real_label)\n",
    "            hvals = discriminator.forward(fake_images).squeeze()\n",
    "            lossG = loss_fn(hvals, labels)\n",
    "            lossG.backward()\n",
    "\n",
    "            # Update the generator weights\n",
    "            optimG.step()\n",
    "            \n",
    "            lossD = lossD_real + lossD_fake\n",
    "            epoch_lossD += lossD\n",
    "            epoch_lossG += lossG\n",
    "        \n",
    "    epoch_lossD = epoch_lossD / len(trainset)\n",
    "    epoch_lossG = epoch_lossG / len(trainset)\n",
    "    print(f'\\n  Discriminator Loss: {epoch_lossD:.3f}  Generator Loss: {epoch_lossG:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train discriminator with a single batch\n",
    "### Maximize log(D(x)) and log(1 - D(G(z)))\n",
    "real_images, _ = next(iter(trainloader))\n",
    "real_images = real_images.to(DEVICE)\n",
    "with torch.enable_grad():\n",
    "    optimD.zero_grad()\n",
    "    \n",
    "    # Train with real images\n",
    "    labels = torch.full((BATCH_SIZE,), real_label, device=DEVICE)\n",
    "    hvals = discriminator.forward(real_images).squeeze()\n",
    "    print(hvals.size())\n",
    "    loss_real = loss_fn(hvals, labels)\n",
    "    loss_real.backward()\n",
    "    \n",
    "    # Train with fake images generated from random noise\n",
    "    noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=DEVICE)\n",
    "    fake_images = generator(noise)\n",
    "    labels.fill_(fake_label)\n",
    "    hvals = discriminator(fake_images.detach()).squeeze()\n",
    "    loss_fake = loss_fn(hvals, labels)\n",
    "    loss_fake.backwards()\n",
    "    \n",
    "    # Update the weights of the discriminator\n",
    "    optimD.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train generator with a single batch\n",
    "#### Maximize log(D(G(z)))\n",
    "with torch.enable_grad():\n",
    "    optimG.zero_grad()\n",
    "    labels.fill_(real_label)\n",
    "    hvals = discriminator.forward(fake_images).squeeze()\n",
    "    loss = loss_fn(hvals, labels)\n",
    "    loss.backward()\n",
    "    optimG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
