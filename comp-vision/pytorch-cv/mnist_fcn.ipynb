{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import plotter as pltr\n",
    "from copy import copy\n",
    "pltr.set_backend(pltr.MatplotlibBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot images\n",
    "white = pltr.Color(red=255, green=255, blue=255)\n",
    "big_white_font = pltr.Font(color=white, size=14)\n",
    "    \n",
    "def rehydrate(img):\n",
    "    img = copy(img.numpy())\n",
    "    img = img.squeeze()  # Get rid of the single channel dim\n",
    "    img = (img * 0.5) + 0.5  # Denormalize\n",
    "    img = 255 * img  # Re-scale to 0-255\n",
    "    return img\n",
    "    \n",
    "def show_imgs(imgs, titles):\n",
    "    frame = pltr.Frame()\n",
    "    frame.layout(1, len(imgs))  # Show all the images in a single row\n",
    "    for img, title in zip(imgs, titles):\n",
    "        chart = frame.create_chart()\n",
    "        chart.title = title\n",
    "        chart.title_font = big_white_font\n",
    "        chart.show_axes = False\n",
    "        imgplot = pltr.GrayscaleImage(img)\n",
    "        chart.add(imgplot)\n",
    "    frame.show()\n",
    "    \n",
    "def show_classification(img, label, probs):\n",
    "    frame = pltr.Frame()\n",
    "    frame.layout(1, 2)\n",
    "    \n",
    "    # Image chart\n",
    "    chart = frame.create_chart()\n",
    "    chart.title = label\n",
    "    chart.title_font = big_white_font\n",
    "    chart.show_axes = False\n",
    "    imgplot = pltr.GrayscaleImage(img)\n",
    "    chart.add(imgplot)\n",
    "    \n",
    "    # Probs plot\n",
    "    chart = frame.create_chart()\n",
    "    chart.title = 'Class Probabilities'\n",
    "    chart.title_font = big_white_font\n",
    "    chart.x_axis.limits = (0, 1)\n",
    "    chart.x_axis.font = pltr.Font(color=white)\n",
    "    chart.y_axis.font = pltr.Font(color=white)\n",
    "    cats = [str(v) for v in range(len(probs))]\n",
    "    probsplot = pltr.HorizontalBar(categories=cats, values=probs)\n",
    "    chart.add(probsplot)\n",
    "    \n",
    "    frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are grayscale images so we only need to normalize the single channel\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "trainset = datasets.MNIST('/data/pytorch/mnist/', download=True, train=True, transform=xforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST('/data/pytorch/mnist/', download=True, train=False, transform=xforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Both images and labels are of type Tensor\n",
    "# Batch of 64 instances. Images are 1x28x28.\n",
    "# Based on our transforms, each image was first scaled to be between 0 and 1 and then normalized\n",
    "# to have 0.5 and 0.5 stddev\n",
    "images, labels = next(iter(trainloader))\n",
    "print(type(images), type(labels))\n",
    "ndx = 0\n",
    "image = images[0]\n",
    "label = labels[0]\n",
    "print(image.size(), label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = rehydrate(image)\n",
    "show_imgs([img], [label.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x\n",
    "    \n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Weights are automatically initialized\n",
    "print(model.fc1.weight.size())\n",
    "print(model.fc1.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I like setting my biases to zero\n",
    "# Note how I am using .data.fill here because fc1.bias is an autograd variable and as such we\n",
    "# cannot mutate its value directly\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten the 2D image into a single vector, but keep the channel dim, do not squeeze it out\n",
    "images.resize_(64, 784)\n",
    "\n",
    "ndx = 0\n",
    "ps = model.forward(images[0])\n",
    "img = rehydrate(images[0].view(28, 28))\n",
    "probs = ps.detach().numpy().squeeze()\n",
    "show_classification(img, labels[0].item(), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all we are doing in the Network class is pass a tensor sequentially through, PyTorch provides an easier way to define such networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.Softmax(dim=0)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "probs = model.forward(images[0]).detach().numpy().squeeze()\n",
    "img = rehydrate(images[0].view(28, 28))\n",
    "lbl = labels[0].item()\n",
    "show_classification(img, lbl, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (logits): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(784, 128)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(128, 64)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('logits', nn.Linear(64, 10))\n",
    "]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=128, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_wts = copy(model.fc1.weight[:3, :3].detach().numpy())\n",
    "print('Initial weights - ', init_wts)\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "optimizer.zero_grad()\n",
    "output = model.forward(images)\n",
    "loss = loss_fn(output, labels)\n",
    "loss.backward()\n",
    "grads = copy(model.fc1.weight.grad[:3, :3].detach().numpy())\n",
    "print('Gradient - ', grads)\n",
    "\n",
    "# This does the actual weight update\n",
    "# If z is the output (or the leaf) variable in the compute graph\n",
    "# and x is one of the roots, then doing a z.backward will calculate\n",
    "# all the derivates of dz/d* for all of z's ancestors but we cannot access the intermediate grads\n",
    "# x.grad will give us dz/dx, i.e., the gradient of x\n",
    "# In the loss function, the parameters are the roots, so parameters.grad will give us dL/dW\n",
    "# Optimizers use this grad state to update the parameters in the usual way: W := W - α(dL/dW)\n",
    "optimizer.step()\n",
    "\n",
    "# Now we should be able to see the new weights\n",
    "print('Updated weights - ', model.fc1.weight[:3, :3])\n",
    "exp_wts = init_wts - 0.01 * grads\n",
    "print('Expected weights - ', exp_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "epochs = 3\n",
    "print_every = 40\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    tot_loss = 0\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Getting batch size from batch\n",
    "        # instead of using 64 because the last batch might be smaller than 64\n",
    "        images.resize_(images.size()[0], 784) \n",
    "        \n",
    "        # Ensure that the param grads are zero'ed out otherwise they'll be accumulated\n",
    "        # with every train step\n",
    "        optimizer.zero_grad()  \n",
    "        output = model.forward(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tot_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            avg_loss = tot_loss / print_every\n",
    "            print(f'Epoch: {e}/{epochs}\\tLoss: {avg_loss:.3f}')\n",
    "            tot_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "image = images[0].view(1, 784)  # Create a \"batch\" of one\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(image)\n",
    "print(logits.size())\n",
    "probs = F.softmax(logits, dim=1).detach().numpy().squeeze()\n",
    "img = rehydrate(image.reshape(28, 28))\n",
    "show_classification(img, labels[0].item(), probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "batch_size = images.size()[0]\n",
    "images = images.resize_(batch_size, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds = []\n",
    "tot_test_size = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        batch_size = images.size()[0]\n",
    "        tot_test_size += batch_size\n",
    "        images.resize_(batch_size, 784)\n",
    "        logits = model.forward(images)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        for image, label, pred in zip(images, labels, preds):\n",
    "            if label != pred:\n",
    "                wrong_preds.append((image, label, pred))\n",
    "acc = (tot_test_size - len(wrong_preds)) / tot_test_size\n",
    "print(f'Accuracy: {acc:.3f}\\tMisclassifications: {len(wrong_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds_sample = random.sample(wrong_preds, k=5)\n",
    "for image, label, pred in wrong_preds_sample:\n",
    "    img = rehydrate(image.view(28, 28))\n",
    "    title = f'Actual: {label}, Predicted {pred}'\n",
    "    show_imgs([img], [title])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
