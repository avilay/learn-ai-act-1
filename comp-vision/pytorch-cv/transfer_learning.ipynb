{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from datetime import datetime\n",
    "from copy import deepcopy, copy\n",
    "import numpy as np\n",
    "# import plotter as pltr\n",
    "# pltr.set_backend(pltr.MatplotlibBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot images\n",
    "white = pltr.Color(red=255, green=255, blue=255)\n",
    "big_white_font = pltr.Font(color=white, size=14)\n",
    "\n",
    "def show_imgs(imgs, titles):\n",
    "    frame = pltr.Frame(height_px=800, width_px=2000)\n",
    "    frame.layout(1, len(imgs))  # Show all the images in a single row\n",
    "    for img, title in zip(imgs, titles):\n",
    "        chart = frame.create_chart()\n",
    "        chart.title = title\n",
    "        chart.title_font = big_white_font\n",
    "        chart.show_axes = False\n",
    "        imgplot = pltr.GrayscaleImage(img)\n",
    "        chart.add(imgplot)\n",
    "    frame.show()\n",
    "    \n",
    "def tensor2img(tensor):\n",
    "    img = copy(tensor.numpy())\n",
    "    img = img.transpose((1, 2, 0))\n",
    "    img = img * stds + means\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = 255 * img\n",
    "    img = img.astype(np.int)\n",
    "    return img\n",
    "\n",
    "def plot_learning_curves(metrics):\n",
    "    frame = pltr.Frame(height_px=1100, width_px=2000)\n",
    "    frame.layout(len(metrics.keys()), 1)\n",
    "    train_color = pltr.Color(red=7, green=87, blue=124)\n",
    "    val_color = pltr.Color(red=124, green=7, blue=87)\n",
    "    for metric, values in metrics.items():\n",
    "        chart = frame.create_chart()\n",
    "        chart.title = metric.upper()\n",
    "        chart.title_font = big_white_font\n",
    "        chart.x_axis = pltr.Axis(label='Epochs', font=pltr.Font(color=white))\n",
    "        chart.y_axis.font = pltr.Font(color=white)\n",
    "        chart.legend_location = pltr.LegendLocation.BEST\n",
    "        \n",
    "        epochs = [str(i) for i in range(len(values['train']))]\n",
    "        \n",
    "        train_metrics = values['train']\n",
    "        train_line = pltr.Line(categories=epochs, values=train_metrics, legend='Train', color=train_color)\n",
    "        chart.add(train_line)\n",
    "        \n",
    "        val_metrics = values['val']\n",
    "        val_line = pltr.Line(categories=epochs, values=val_metrics, legend='Val', color=val_color)\n",
    "        chart.add(val_line)\n",
    "    frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View without any transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datadir = '/data/pytorch/bees_ants/train'\n",
    "traindata = datasets.ImageFolder(train_datadir)\n",
    "print(traindata[0])\n",
    "print(len(traindata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "imgs = []\n",
    "for i in range(5):\n",
    "    pil_img, label = traindata[i]\n",
    "    imgs.append(np.array(pil_img))\n",
    "    titles.append(traindata.classes[label])\n",
    "show_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "imgs = []\n",
    "for i in range(-1, -6, -1):\n",
    "    pil_img, label = traindata[i]\n",
    "    imgs.append(np.array(pil_img))\n",
    "    titles.append(traindata.classes[label])\n",
    "show_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_xforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "val_xforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "train_datadir = '/data/hymenoptera_data/train'\n",
    "traindata = datasets.ImageFolder(train_datadir, train_xforms)\n",
    "trainloader = DataLoader(traindata, batch_size=4, shuffle=True)\n",
    "\n",
    "val_datadir = '/data/hymenoptera_data/val'\n",
    "valdata = datasets.ImageFolder(val_datadir, val_xforms)\n",
    "valloader = DataLoader(valdata, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "imgs = [tensor2img(image) for image in images]\n",
    "titles = [traindata.classes[label] for label in labels]\n",
    "show_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for batch in trainloader:\n",
    "    num += batch[0].size(0)\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, lr, num_epochs=25):\n",
    "    start = datetime.now()\n",
    "    metrics = {\n",
    "        'loss': {\n",
    "            'val': [],\n",
    "            'train': []\n",
    "        },\n",
    "        'acc': {\n",
    "            'val': [],\n",
    "            'train': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        lr.step()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            with torch.enable_grad():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                train_acc += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_train_loss = train_loss / len(traindata)\n",
    "        epoch_train_acc = train_acc.double() / len(traindata)\n",
    "        metrics['loss']['train'].append(epoch_train_loss)\n",
    "        metrics['acc']['train'].append(epoch_train_acc.item())\n",
    "        print(f'Train - Loss: {epoch_train_loss:.3f}\\tAcc: {epoch_train_acc:.3f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0\n",
    "        for images, labels in valloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_acc += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(valdata)\n",
    "        epoch_val_acc = val_acc.double() / len(valdata)\n",
    "        metrics['loss']['val'].append(epoch_val_loss)\n",
    "        metrics['acc']['val'].append(epoch_val_acc.item())\n",
    "        print(f'Val - Loss: {epoch_val_loss:.3f}\\tAcc: {epoch_val_acc:.3f}')\n",
    "    \n",
    "    end = datetime.now()\n",
    "    print(f'Training completed in {end-start}')\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Decay learning rate by a factor of 0.1 every 7 epochs\n",
    "exp_lr = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.547\tAcc: 0.697\n",
      "Val - Loss: 0.344\tAcc: 0.869\n",
      "\n",
      "Epoch 2/25\n",
      "Train - Loss: 0.473\tAcc: 0.816\n",
      "Val - Loss: 0.232\tAcc: 0.908\n",
      "\n",
      "Epoch 3/25\n",
      "Train - Loss: 0.443\tAcc: 0.816\n",
      "Val - Loss: 0.274\tAcc: 0.889\n",
      "\n",
      "Epoch 4/25\n",
      "Train - Loss: 0.365\tAcc: 0.844\n",
      "Val - Loss: 0.301\tAcc: 0.895\n",
      "\n",
      "Epoch 5/25\n",
      "Train - Loss: 0.500\tAcc: 0.824\n",
      "Val - Loss: 0.247\tAcc: 0.922\n",
      "\n",
      "Epoch 6/25\n",
      "Train - Loss: 0.461\tAcc: 0.824\n",
      "Val - Loss: 0.381\tAcc: 0.869\n",
      "\n",
      "Epoch 7/25\n",
      "Train - Loss: 0.377\tAcc: 0.840\n",
      "Val - Loss: 0.321\tAcc: 0.902\n",
      "\n",
      "Epoch 8/25\n",
      "Train - Loss: 0.314\tAcc: 0.852\n",
      "Val - Loss: 0.272\tAcc: 0.908\n",
      "\n",
      "Epoch 9/25\n",
      "Train - Loss: 0.265\tAcc: 0.902\n",
      "Val - Loss: 0.266\tAcc: 0.902\n",
      "\n",
      "Epoch 10/25\n",
      "Train - Loss: 0.303\tAcc: 0.881\n",
      "Val - Loss: 0.235\tAcc: 0.908\n",
      "\n",
      "Epoch 11/25\n",
      "Train - Loss: 0.290\tAcc: 0.885\n",
      "Val - Loss: 0.229\tAcc: 0.915\n",
      "\n",
      "Epoch 12/25\n",
      "Train - Loss: 0.319\tAcc: 0.865\n",
      "Val - Loss: 0.233\tAcc: 0.895\n",
      "\n",
      "Epoch 13/25\n",
      "Train - Loss: 0.213\tAcc: 0.910\n",
      "Val - Loss: 0.211\tAcc: 0.935\n",
      "\n",
      "Epoch 14/25\n",
      "Train - Loss: 0.259\tAcc: 0.902\n",
      "Val - Loss: 0.223\tAcc: 0.922\n",
      "\n",
      "Epoch 15/25\n",
      "Train - Loss: 0.248\tAcc: 0.910\n",
      "Val - Loss: 0.219\tAcc: 0.922\n",
      "\n",
      "Epoch 16/25\n",
      "Train - Loss: 0.251\tAcc: 0.881\n",
      "Val - Loss: 0.202\tAcc: 0.935\n",
      "\n",
      "Epoch 17/25\n",
      "Train - Loss: 0.270\tAcc: 0.865\n",
      "Val - Loss: 0.219\tAcc: 0.922\n",
      "\n",
      "Epoch 18/25\n",
      "Train - Loss: 0.271\tAcc: 0.885\n",
      "Val - Loss: 0.252\tAcc: 0.902\n",
      "\n",
      "Epoch 19/25\n",
      "Train - Loss: 0.260\tAcc: 0.889\n",
      "Val - Loss: 0.220\tAcc: 0.935\n",
      "\n",
      "Epoch 20/25\n",
      "Train - Loss: 0.257\tAcc: 0.893\n",
      "Val - Loss: 0.195\tAcc: 0.935\n",
      "\n",
      "Epoch 21/25\n",
      "Train - Loss: 0.254\tAcc: 0.898\n",
      "Val - Loss: 0.243\tAcc: 0.895\n",
      "\n",
      "Epoch 22/25\n",
      "Train - Loss: 0.249\tAcc: 0.898\n",
      "Val - Loss: 0.232\tAcc: 0.902\n",
      "\n",
      "Epoch 23/25\n",
      "Train - Loss: 0.284\tAcc: 0.881\n",
      "Val - Loss: 0.224\tAcc: 0.908\n",
      "\n",
      "Epoch 24/25\n",
      "Train - Loss: 0.239\tAcc: 0.893\n",
      "Val - Loss: 0.202\tAcc: 0.935\n",
      "\n",
      "Epoch 25/25\n",
      "Train - Loss: 0.251\tAcc: 0.893\n",
      "Val - Loss: 0.209\tAcc: 0.922\n",
      "Training completed in 0:02:54.091498\n"
     ]
    }
   ],
   "source": [
    "model, metrics = train(model, loss_fn, optimizer, exp_lr, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "preds = torch.argmax(model(images.to(DEVICE)), dim=1)\n",
    "imgs = [tensor2img(image) for image in images]\n",
    "titles = [f'Predicted: {valdata.classes[pred]} Actual: {valdata.classes[label]}' for label, pred in zip(labels, preds)]\n",
    "show_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings from Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_featuers = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # This will have requires_grad TRUE\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "exp_lr = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = train(model, loss_fn, optimizer, exp_lr, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "preds = torch.argmax(model(images.to(DEVICE)), dim=1)\n",
    "imgs = [tensor2img(image) for image in images]\n",
    "titles = [f'Predicted: {valdata.classes[pred]} Actual: {valdata.classes[label]}' for label, pred in zip(labels, preds)]\n",
    "show_imgs(imgs, titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
