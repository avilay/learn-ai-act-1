{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding of Words\n",
    "\n",
    "The purpose of this technique is to transform a list of documents into a tensor (either a vector or a matrix) that can then be fed into a Neural Net. The output of this process can either be a 2D matrix or a 3D tensor. Both of these are binary, i.e, they only have 0s and 1s as their elements. The *hyper parameters* to this process are -\n",
    "\n",
    "* The tokenization scheme (single words, 2-grams, 3-grams, etc.)\n",
    "* Maximum size of the vocabulary (*v*)\n",
    "* [Optionally] The maximum document length (in number of tokens) (*n*)\n",
    "\n",
    "#### Create the vocabulary\n",
    "Create a frequency distribution of tokens. Assign each token a unique id or index. Typically, the tokens are given indexes depending on their frequency in the corpus. So the most frequent token will have index 1, the next most frequent token will have index 2, and so on. The output of this step are two maps - `tokens` which, given an index, gives the corresponding token at that index, i.e., `tokens[index] = token_at_index`. And `indexes` which, given a token, gives the index of that token, i.e., `indexes[token] = index_of_token`. Note, index 0 is not assigned to anything.\n",
    "\n",
    "The final vocabulary is supposed to contain the top *v* tokens. One way to implement this is to only consider tokens whose indexes are less than *v* and drop everything else. This is however, not done at this step.\n",
    "\n",
    "#### Transform documents into integer vectors\n",
    "Replace each token in the document with its index. Then drop the indexes that are not part of the vocabulary. Each document vector will potentially have a different length. \n",
    "\n",
    "#### Transform documents into a 3D tensor\n",
    "Each document is first truncated to the first *n* indexes. Then each index is replaced with a one-hot vector that is *v* in size. All elements of this vector are 0 except the element at index which is 1. Think of this as transforming each document into a matrix of *n* x *v*. Each row of this document matrix is the one-hot vector of the word at that position. If a word occurs multiple times (say at position *i* and *j*), the same one-hot vector will be repeated at rows *i* and *j*. This is done for all documents resulting in a tensor that is *m x n x v* in size.\n",
    "\n",
    "#### Transform documents into a 2D matrix\n",
    "As an alternate representation, instead of a 3D tensor, the output is a 2D matrix. Here there is no need to truncate the document. Each document is represented as a vector *v* in size where all the elements are 0 except indexes that appear in that document, which are set to 1. In this representation, neither the position of the word nor its frequency are relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sample documents\n",
    "In order to better understand how word embeddings work, it is nice to have a synthetic set of documents with specific words and frequencies. Let us decide on having 3 documents and 10 words in the entire corpus. Let the 10 words be *pedantic, fruit, ornament, magic, laptop, ipad, book, console, piano, hugs*. And let the total frequency of these words in the entire corpus (and the documents) be as shown in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedantic ornament magic pedantic pedantic piano book ipad piano hugs laptop ipad magic console ornament laptop ipad 17\n",
      "\n",
      "console fruit laptop ipad fruit fruit book laptop pedantic laptop pedantic 11\n",
      "\n",
      "magic magic pedantic console book fruit pedantic fruit ornament pedantic magic fruit ornament console ornament magic pedantic fruit ornament book magic ornament ornament fruit 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents\n",
    "word_freqs = {\n",
    "    'pedantic': (3, 2, 4),\n",
    "    'fruit': (0, 3, 5),\n",
    "    'ornament': (2, 0, 6),\n",
    "    'magic': (2, 0, 5),\n",
    "    'laptop': (2, 3, 0),\n",
    "    'ipad': (3, 1, 0),\n",
    "    'book': (1, 1, 2),\n",
    "    'console': (1, 1, 2),\n",
    "    'piano': (2, 0, 0),\n",
    "    'hugs': (1, 0, 0),\n",
    "}\n",
    "\n",
    "raw_docs = [[], [], []]\n",
    "for i in range(len(raw_docs)):\n",
    "    for word, freqs in word_freqs.items():\n",
    "        words = [word] * freqs[i]\n",
    "        raw_docs[i] += words\n",
    "\n",
    "docs = []\n",
    "for raw_doc in raw_docs:\n",
    "    np.random.shuffle(raw_doc)\n",
    "    doc = ' '.join(raw_doc)\n",
    "    docs.append(doc)\n",
    "\n",
    "tp = [print(doc, len(doc.split()), end='\\n\\n') for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'pedantic', 'ornament', 'fruit', 'magic', 'laptop', 'book', 'ipad', 'console', 'piano', 'hugs']\n",
      "{'pedantic': 1, 'ornament': 2, 'fruit': 3, 'magic': 4, 'laptop': 5, 'book': 6, 'ipad': 7, 'console': 8, 'piano': 9, 'hugs': 10}\n"
     ]
    }
   ],
   "source": [
    "# Create the vocuabulary \n",
    "from collections import Counter\n",
    "\n",
    "def create_vocab(docs):\n",
    "    all_tokens = []\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            all_tokens.append(word)\n",
    "    \n",
    "    word_freqs = Counter(all_tokens)\n",
    "    tokens = []\n",
    "    for word, freq in word_freqs.most_common():\n",
    "        tokens.append(word)\n",
    "    tokens = [None] + tokens\n",
    "    \n",
    "    indexes = {token: index for index, token in enumerate(tokens)}\n",
    "    del indexes[None]\n",
    "    \n",
    "    return tokens, indexes\n",
    "\n",
    "tokens, indexes = create_vocab(docs)\n",
    "print(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pedantic ornament magic pedantic pedantic piano book ipad piano hugs laptop ipad magic console ornament laptop ipad\n",
      "[1, 2, 4, 1, 1, 9, 6, 7, 9, 10, 5, 7, 4, 8, 2, 5, 7]\n",
      "[1, 2, 4, 1, 1, 6, 5, 4, 2, 5] 10\n",
      "\n",
      "\n",
      "console fruit laptop ipad fruit fruit book laptop pedantic laptop pedantic\n",
      "[8, 3, 5, 7, 3, 3, 6, 5, 1, 5, 1]\n",
      "[3, 5, 3, 3, 6, 5, 1, 5, 1] 9\n",
      "\n",
      "\n",
      "magic magic pedantic console book fruit pedantic fruit ornament pedantic magic fruit ornament console ornament magic pedantic fruit ornament book magic ornament ornament fruit\n",
      "[4, 4, 1, 8, 6, 3, 1, 3, 2, 1, 4, 3, 2, 8, 2, 4, 1, 3, 2, 6, 4, 2, 2, 3]\n",
      "[4, 4, 1, 6, 3, 1, 3, 2, 1, 4, 3, 2, 2, 4, 1, 3, 2, 6, 4, 2, 2, 3] 22\n"
     ]
    }
   ],
   "source": [
    "# Transform documents to vectors\n",
    "raw_vecs = [[indexes[token] for token in doc.split()] for doc in docs]\n",
    "\n",
    "v = 7\n",
    "vecs = [[indexes[token] for token in doc.split() if indexes[token] < v] for doc in docs]\n",
    "\n",
    "for doc, raw_vec, vec in zip(docs, raw_vecs, vecs):\n",
    "    print('\\n')\n",
    "    print(doc)\n",
    "    print(raw_vec)\n",
    "    print(vec, len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Create 3D tensor\n",
    "m = len(docs)\n",
    "n = 15\n",
    "X_3d = np.zeros((m, n, v))\n",
    "for i, vec in enumerate(vecs):\n",
    "    for j, index in enumerate(vec[:n]):\n",
    "        X_3d[i, j, index] = 1\n",
    "print(X_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1.  0.  1.  1.  1.]\n",
      " [ 0.  1.  0.  1.  0.  1.  1.]\n",
      " [ 0.  1.  1.  1.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create 2D matrix\n",
    "X_2d = np.zeros((m, v))\n",
    "for i, vec in enumerate(vecs):\n",
    "    for index in vec:\n",
    "        X_2d[i, index] = 1\n",
    "print(X_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras for one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D matrix representation\n",
    "Keras has in-built support for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pedantic': 1, 'ornament': 2, 'fruit': 3, 'magic': 4, 'laptop': 5, 'book': 6, 'ipad': 7, 'console': 8, 'piano': 9, 'hugs': 10}\n",
      "[None, 'pedantic', 'ornament', 'fruit', 'magic', 'laptop', 'book', 'ipad', 'console', 'piano', 'hugs']\n",
      "\n",
      "\n",
      "pedantic ornament magic pedantic pedantic piano book ipad piano hugs laptop ipad magic console ornament laptop ipad\n",
      "[1, 2, 4, 1, 1, 9, 6, 7, 9, 10, 5, 7, 4, 8, 2, 5, 7]\n",
      "[1, 2, 4, 1, 1, 6, 5, 4, 2, 5]\n",
      "\n",
      "\n",
      "console fruit laptop ipad fruit fruit book laptop pedantic laptop pedantic\n",
      "[8, 3, 5, 7, 3, 3, 6, 5, 1, 5, 1]\n",
      "[3, 5, 3, 3, 6, 5, 1, 5, 1]\n",
      "\n",
      "\n",
      "magic magic pedantic console book fruit pedantic fruit ornament pedantic magic fruit ornament console ornament magic pedantic fruit ornament book magic ornament ornament fruit\n",
      "[4, 4, 1, 8, 6, 3, 1, 3, 2, 1, 4, 3, 2, 8, 2, 4, 1, 3, 2, 6, 4, 2, 2, 3]\n",
      "[4, 4, 1, 6, 3, 1, 3, 2, 1, 4, 3, 2, 2, 4, 1, 3, 2, 6, 4, 2, 2, 3]\n",
      "\n",
      " [[ 0.  1.  1.  0.  1.  1.  1.]\n",
      " [ 0.  1.  0.  1.  0.  1.  1.]\n",
      " [ 0.  1.  1.  1.  1.  0.  1.]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# del indexes\n",
    "# del tokens\n",
    "# del vecs\n",
    "# del raw_vecs\n",
    "# del X_2d\n",
    "\n",
    "tokenizer = Tokenizer(num_words=v)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "# Keras only has the indexes map, so create the tokens map by hand\n",
    "indexes_k = tokenizer.word_index\n",
    "tokens_k = [None] * (len(indexes_k) + 1)\n",
    "for word, index in indexes_k.items():\n",
    "    tokens_k[index] = word\n",
    "print(indexes_k)\n",
    "print(tokens_k)\n",
    "\n",
    "raw_vecs_k = [[indexes_k[token] for token in doc.split()] for doc in docs]\n",
    "vecs_k = tokenizer.texts_to_sequences(docs)\n",
    "for doc, raw_vec, vec in zip(docs, raw_vecs_k, vecs_k):\n",
    "    print('\\n')\n",
    "    print(doc)\n",
    "    print(raw_vec)\n",
    "    print(vec)\n",
    "\n",
    "X_2d_k = tokenizer.texts_to_matrix(docs)\n",
    "print('\\n', X_2d_k)\n",
    "print(np.array_equal(X_2d, X_2d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "The astute reader will have observed that there is a off-by-one bug in the code above. Even though I wanted the top 7 words in the vocab, I am only getting the top 6 words because the first token vocab is always None. To keep things consistent with Keras, I have deliberately introduced this bug in my code in cell 4 line 6, the less-than sign should be replaced with the less-than-equal-to sign -\n",
    "```\n",
    "vecs = [[indexes[token] for token in doc.split() if indexes[token] <= v] for doc in docs]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3D tensor representation\n",
    "Keras does not have in-built support for this, so need to do some pre-processing. First convert all document vectors into equal size of length *n*. Then turn each same-length document vector into 2D matrix of size *n* x *v*. Do this for all the *m* documents to get a tensor of size *m* x *n* x *v*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our documents are of unequal lengths - 17, 11, and 24 and after throwing some tokens (using *v* as 7) we end up document vectors that are 12, 9, and 20 in size. Lets choose *n* to be 15. This means that the first and second documents will have to be padded and the third document will have to be cut. By default the Keras padder pads/cuts from the front. But to show equivalence with the earlier 3D tensor, we will ask Keras to trucnate/pad from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "22\n",
      "[1 2 4 1 1 6 5 4 2 5]\n",
      "[1 2 4 1 1 6 5 4 2 5 0 0 0 0 0]\n",
      "\n",
      "\n",
      "[3 5 3 3 6 5 1 5 1]\n",
      "[3 5 3 3 6 5 1 5 1 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "[4 4 1 6 3 1 3 2 1 4 3 2 2 4 1 3 2 6 4 2 2 3]\n",
      "[4 4 1 6 3 1 3 2 1 4 3 2 2 4 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "v = 7\n",
    "tokenizer = Tokenizer(num_words=v)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "indexes = tokenizer.word_index\n",
    "tokens = {index: word for word, index in indexes.items()}\n",
    "\n",
    "vecs = tokenizer.texts_to_sequences(docs)\n",
    "tp = [print(len(vec)) for vec in vecs]\n",
    "padded_vecs = pad_sequences(vecs, maxlen=15, padding='post', truncating='post')\n",
    "for padded_vec, vec in zip(padded_vecs, vecs):\n",
    "    print(np.array(vec))  # Converting to nupy array for better printing\n",
    "    print(padded_vec)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take each same-length document vector and turn it into a 2D matrix. Do this for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "n = 15  # All vecs are of this length\n",
    "v = 7\n",
    "def vec2mat(vec):\n",
    "    mat = []\n",
    "    for index in vec:\n",
    "        one_hot_vec = np.zeros(v)\n",
    "        if index > 0:\n",
    "            one_hot_vec[index] = 1\n",
    "        mat.append(one_hot_vec)\n",
    "    return np.array(mat)\n",
    "\n",
    "X_3d_k = []\n",
    "for padded_vec in padded_vecs:\n",
    "    mat = vec2mat(padded_vec)\n",
    "    X_3d_k.append(mat)\n",
    "X_3d_k = np.array(X_3d_k)\n",
    "print(X_3d_k)\n",
    "print(np.array_equal(X_3d, X_3d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "Take the top 10,000 words/tokens as part of the vocabulary. Note, that the imdb dataset does not discard words that are not in the vocabulary. Instead, it replaces them with the special *unknown* token.\n",
    "\n",
    "Take the first 30 words in each review. If a review has less than 20 words, pad the rest. This includes *unknown* tokens. So if the word vector had a token *unknown*, it would be counted when taking the first 30 words. Remember Keras by default pads/truncate from the front. I have to explicitly ask it to pad/truncate from the back so as to take the first 30 words instead of the last 30 words. In reality it does not matter where I pad/truncate, but because I have been examining this dataset from the front, I'll continue to pad/truncate from the front.\n",
    "\n",
    "In the first sample, index for the *unknown* token (2) is included in the padded sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 10000\n",
    "n = 30\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4\n",
      "  173   36  256    5   25  100   43  838  112   50  670    2    9   35  480]\n",
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4\n",
      "  173   36  256    5   25  100   43  838  112   50  670    2    9   35  480]\n"
     ]
    }
   ],
   "source": [
    "x_train_padded = pad_sequences(x_train, maxlen=n, padding='post', truncating='post')\n",
    "x_test_padded = pad_sequences(x_test, maxlen=n, padding='post', truncating='post')\n",
    "print(np.array(x_train[0])[:30])\n",
    "print(x_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the embeddings from the input data\n",
    "The Embedding layer converts a document vector (or any vector of integers) of size *n* into a dense 2D matrix of size *n* x *d*. Here *n* is the fixed size of the document, and *d* is the dimensionality of the embedding, i.e., each **word (index)** will be converted into a dense float vector of size *d*. Also, each element of the incoming document vector is a token index in the range [1, *v*). So all the *m* documents will be converted into a 3D tensor of size *m* x *n* x *d*. Notice the similarity between embeddings and 3D tensor representation of one-hot vectors.\n",
    "\n",
    "The Embedding layer needs two manadatory arguments - the length of the vocabulary *v* and the output dimensionality *d*. If the Embedding layer is the first layer in the network (as it usually is) we need to provide the input_shape of the incoming sample. The Embedding layer also accepts a named argument called `input_length` instead `input_shape` if the input is going to be 1D. So `input_length=n` is equivalent to `input_shape=(n,)`. I prefer to use input_shape to keep things consistent.\n",
    "\n",
    "Each 2D document of size *n* x *d* then needs to be flattened. The Flatten layer will take each row, starting from the first row, and lay them side-by-side to create a big row vector. The output of the Flatten layer is a row vector of size *n*.*d*.\n",
    "\n",
    "Finally, add a Dense single unit classification layer with sigmoid activation so the network will learn to classify a review as either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 241       \n",
      "=================================================================\n",
      "Total params: 80,241\n",
      "Trainable params: 80,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(v, 8, input_shape=(n,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.6755 - acc: 0.6005 - val_loss: 0.6391 - val_acc: 0.6688\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.5736 - acc: 0.7243 - val_loss: 0.5642 - val_acc: 0.7102\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4915 - acc: 0.7733 - val_loss: 0.5352 - val_acc: 0.7252\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4430 - acc: 0.7973 - val_loss: 0.5284 - val_acc: 0.7324\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4085 - acc: 0.8179 - val_loss: 0.5322 - val_acc: 0.7336\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3805 - acc: 0.8344 - val_loss: 0.5389 - val_acc: 0.7352\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3550 - acc: 0.8482 - val_loss: 0.5487 - val_acc: 0.7372\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3313 - acc: 0.8595 - val_loss: 0.5602 - val_acc: 0.7380\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3094 - acc: 0.8732 - val_loss: 0.5706 - val_acc: 0.7346\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2881 - acc: 0.8848 - val_loss: 0.5831 - val_acc: 0.7308\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_padded, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a final validation accuracy of 73%. Interestingly enough if I pad/truncate from the start (instead from the end as I have done so far) the validation accuracy improves to around 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_padded_pre = pad_sequences(x_train, maxlen=n)\n",
    "x_test_padded_pre = pad_sequences(x_test, maxlen=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.5345 - acc: 0.7472 - val_loss: 0.5245 - val_acc: 0.7524\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4505 - acc: 0.7929 - val_loss: 0.4991 - val_acc: 0.7616\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4014 - acc: 0.8196 - val_loss: 0.4892 - val_acc: 0.7666\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3668 - acc: 0.8401 - val_loss: 0.4861 - val_acc: 0.7708\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3376 - acc: 0.8556 - val_loss: 0.4871 - val_acc: 0.7722\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.3122 - acc: 0.8703 - val_loss: 0.4913 - val_acc: 0.7710\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2885 - acc: 0.8828 - val_loss: 0.4986 - val_acc: 0.7706\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2665 - acc: 0.8948 - val_loss: 0.5070 - val_acc: 0.7696\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2459 - acc: 0.9048 - val_loss: 0.5180 - val_acc: 0.7672\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2266 - acc: 0.9148 - val_loss: 0.5289 - val_acc: 0.7654\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_padded_pre, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use external embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings matrix\n",
    "Download the Glove embeddings db from http://nlp.stanford.edu/data/glove.6B.zip and extract it in /data/learn-keras. The dataset consists of 400,000 words with the embedding dimension being 50, 100, 200, and 300. Each embedding dimension is in a different file. So the file glove.6B.100d.txt contains the embeddings of all 400,000 words, with each embedding vector being 100 in size.\n",
    "\n",
    "We need to create the embeddings matrix from this db to be used in Keras. The embedding matrix is a\n",
    "matrix of size *v x d* where *v* is the vocabulary size and *d* is the dimenstionality of the embeddings. Each row of the matrix contains the embeddings for the token with that rownum as the index. E.g, `mat[i] = [embedding vector of token with index i]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find embeddings for 203 words\n",
      "[\"else's\", \"miyazaki's\", \"victoria's\", \"paul's\", \"chan's\", \"show's\", \"wife's\", \"character's\", \"hadn't\", \"isn't\"]\n"
     ]
    }
   ],
   "source": [
    "indexes = imdb.get_word_index()\n",
    "embeddings_indexes = {}\n",
    "with open('/data/learn-keras/glove.6B.100d.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        flds = line.split()\n",
    "        word = flds[0]\n",
    "        embeddings_vec = np.array(flds[1:], dtype=np.float32)\n",
    "        embeddings_indexes[word] = embeddings_vec\n",
    "\n",
    "v = 10000\n",
    "d = 100\n",
    "embeddings_matrix = np.zeros((v, d))\n",
    "words_no_embeddings = []\n",
    "for word, index in indexes.items():\n",
    "    if index < v:\n",
    "        if word in embeddings_indexes:\n",
    "            embeddings_vec = embeddings_indexes[word]\n",
    "            embeddings_matrix[index] = embeddings_vec\n",
    "        else:\n",
    "            words_no_embeddings.append(word)\n",
    "print(f'Unable to find embeddings for {len(words_no_embeddings)} words')\n",
    "print(words_no_embeddings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('br', 7),\n",
       " ('in', 8),\n",
       " ('it', 9),\n",
       " ('i', 10)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(indexes.items(), key=lambda kv: kv[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embeddings_matrix[1], embeddings_indexes['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix[indexes['the']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "Define the model with the embeddings layer as usual. But then similar to transfer learning, set the weights of the embeddings layer manually and then freeze it so they won't change during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 3001      \n",
      "=================================================================\n",
      "Total params: 1,003,001\n",
      "Trainable params: 1,003,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "v = 10000\n",
    "n = 30\n",
    "d = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(v, d, input_shape=(n,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.layers[0].set_weights([embeddings_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 7s - loss: 0.6950 - acc: 0.5660 - val_loss: 0.6221 - val_acc: 0.6542\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 7s - loss: 0.5283 - acc: 0.7371 - val_loss: 0.5414 - val_acc: 0.7210\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.4228 - acc: 0.8059 - val_loss: 0.5296 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.3567 - acc: 0.8452 - val_loss: 0.5204 - val_acc: 0.7548\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.3027 - acc: 0.8718 - val_loss: 0.5387 - val_acc: 0.7514\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.2520 - acc: 0.8991 - val_loss: 0.6240 - val_acc: 0.7300\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.2084 - acc: 0.9230 - val_loss: 0.5866 - val_acc: 0.7466\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.1684 - acc: 0.9420 - val_loss: 0.6139 - val_acc: 0.7436\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.1331 - acc: 0.9566 - val_loss: 0.6966 - val_acc: 0.7324\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 6s - loss: 0.1021 - acc: 0.9705 - val_loss: 0.7035 - val_acc: 0.7380\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_padded_pre, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
