{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the sequential model first\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The same model using functional APIs\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.6318     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5359     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5206     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5124     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5082     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5047     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5023     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5002     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4983     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4963     \n",
      "  32/1000 [..............................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Generate dummy numpy data to train on\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input Models\n",
    "\n",
    "## Motivating Example\n",
    "A typical question-answering model has two inputs: a natural language question, and a text snippet (such as a news article) providing information to be used for answering the question. The model must then produce a answer: in the simplest possible setup, this is simply a one-word answer.\n",
    "\n",
    "![arch](images/multi_input_arch.png)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "text (InputLayer)                (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "question (InputLayer)            (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 200, 64)       640000      text[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)         (None, 200, 32)       320000      question[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 32)            12416       embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                   (None, 16)            3136        embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 48)            0           lstm_11[0][0]                    \n",
      "                                                                   lstm_12[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 500)           24500       concatenate_5[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 1,000,052\n",
      "Trainable params: 1,000,052\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocab_size = 10000\n",
    "question_vocab_size = 10000\n",
    "answer_vocab_size = 500\n",
    "\n",
    "# Text input is a variable-length sequence of integers\n",
    "text_input = Input(shape=(200,), dtype='int32', name='text')\n",
    "\n",
    "# Which is embedded into a sequence of vectors of size 64\n",
    "embedded_text = layers.Embedding(text_vocab_size, 64)(text_input)\n",
    "\n",
    "# Which we encode in a single vector via LSTM\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# Same process with the question\n",
    "question_input = Input(shape=(200,), dtype='int32', name='question')\n",
    "embedded_question = layers.Embedding(question_vocab_size, 32)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# We then concatenate the encoded question and the encoded text\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "# And we add a softmax classifier on top\n",
    "answer = layers.Dense(answer_vocab_size, activation='softmax')(concatenated)\n",
    "\n",
    "# At the model instantiation we specify the two inputs and the single output\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 5s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s - loss: 0.0000e+00 - acc: 0.0080     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13afde6a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "max_length = 200\n",
    "\n",
    "# Lets generate some random numpy data\n",
    "text = np.random.randint(1, text_vocab_size, size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocab_size, size=(num_samples, max_length))\n",
    "\n",
    "# Answers are one-hot encoded, not integers\n",
    "answers = np.random.randint(0, 1, size=(num_samples, answer_vocab_size))\n",
    "\n",
    "# Fit using a list of inputs\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)\n",
    "\n",
    "# Fit using a dict of inputs provided the inputs were named\n",
    "# model.fit({'text': text, 'question': question}, answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output Models\n",
    "\n",
    "## Motivating Example\n",
    "A network that attempts to simultaneously predict different properties of the data: lets say, a network that as input a series of social media posts from one single anonymous person, and tries to predict attributes of that person such as age, gender, and income.\n",
    "\n",
    "![arch](images/multi_output_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "posts (InputLayer)               (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)         (None, None, 256)     12800000    posts[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, None, 128)     163968      embedding_19[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, None, 128)     0           conv1d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)               (None, None, 256)     164096      max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)               (None, None, 256)     327936      conv1d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, None, 256)     0           conv1d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)               (None, None, 256)     327936      max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)               (None, None, 256)     327936      conv1d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalMa (None, 256)           0           conv1d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 128)           32896       global_max_pooling1d_4[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "age (Dense)                      (None, 1)             129         dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "income (Dense)                   (None, 10)            1290        dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "gender (Dense)                   (None, 1)             129         dense_14[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 14,146,316\n",
      "Trainable params: 14,146,316\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocab_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(vocab_size, 256)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Note we are giving names to output layers\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])\n",
    "\n",
    "# Even though we are specifying the losses for each head, when optimizing\n",
    "# they all get summed up and then minimized.\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss={\n",
    "        'age': 'mse', \n",
    "        'income': 'categorical_crossentropy', \n",
    "        'gender': 'binary_crossentropy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Can also specify loss weights. Otherwise the head that has the loss\n",
    "# with the greatest magnitude will get optimized better in preference\n",
    "# to the other heads\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss={\n",
    "        'age': 'mse', \n",
    "        'income': 'categorical_crossentropy', \n",
    "        'gender': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'age': 0.25,\n",
    "        'income': 1,\n",
    "        'gender': 10.\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "computed output size would be negative\n\t [[Node: conv1d_20/convolution/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1d_20/convolution/ExpandDims, conv1d_20/convolution/ExpandDims_1)]]\n\nCaused by op 'conv1d_20/convolution/Conv2D', defined at:\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-d3892ccc088c>\", line 16, in <module>\n    x = layers.Conv1D(256, 5, activation='relu')(x)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 156, in call\n    dilation_rate=self.dilation_rate[0])\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3124, in conv1d\n    data_format=tf_data_format)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 116, in _non_atrous_convolution\n    name=scope)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2013, in conv1d\n    data_format=data_format)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): computed output size would be negative\n\t [[Node: conv1d_20/convolution/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1d_20/convolution/ExpandDims, conv1d_20/convolution/ExpandDims_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: computed output size would be negative\n\t [[Node: conv1d_20/convolution/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1d_20/convolution/ExpandDims, conv1d_20/convolution/ExpandDims_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-754bad92b30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     },\n\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: computed output size would be negative\n\t [[Node: conv1d_20/convolution/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1d_20/convolution/ExpandDims, conv1d_20/convolution/ExpandDims_1)]]\n\nCaused by op 'conv1d_20/convolution/Conv2D', defined at:\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-d3892ccc088c>\", line 16, in <module>\n    x = layers.Conv1D(256, 5, activation='relu')(x)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 156, in call\n    dilation_rate=self.dilation_rate[0])\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3124, in conv1d\n    data_format=tf_data_format)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 116, in _non_atrous_convolution\n    name=scope)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2013, in conv1d\n    data_format=data_format)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/avilay/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): computed output size would be negative\n\t [[Node: conv1d_20/convolution/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1d_20/convolution/ExpandDims, conv1d_20/convolution/ExpandDims_1)]]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "maxlen = 200\n",
    "posts = np.random.randint(1, vocab_size, size=(num_samples, maxlen))\n",
    "age_targets = np.random.randint(20, 75, num_samples)\n",
    "income_targets = np.random.randint(0, 1, size=(num_samples, num_income_groups))\n",
    "gender_targets = np.random.randint(0, 1, num_samples)\n",
    "\n",
    "# Can specify the targets positionally\n",
    "# model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=128)\n",
    "\n",
    "# Or use a dict if the target layers were named in the model\n",
    "model.fit(\n",
    "    posts, \n",
    "    {\n",
    "        'age': age_targets,\n",
    "        'income': income_targets,\n",
    "        'gender': gender_targets\n",
    "    },\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGs as Model\n",
    "Inception module\n",
    "\n",
    "![arch](images/dag_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# We assume the existance of a 4D input tensor 'x'\n",
    "\n",
    "# Every branch has the same stride value (2), which is necessary to keep \n",
    "# all branch outputs the same size, for concatenation later on.\n",
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x)\n",
    "\n",
    "# In this branch, the striding occurs in the spatial convolutional layer\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "# In this branch, the striding occurs in the average pooling layer\n",
    "branch_c = layers.AveragePooling2D(3, strides=2, activation='relu')(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Connections\n",
    "\n",
    "A residual connection simply consist of making the output of an earlier layer available as input to a later layer, effectively creating a shortcut in a sequential network. Rather than being concatenated to the later activation, the earlier output is summed with the later activation, which assumes that both activations have the same size. In case of differing sizes, use some sort of linear transformation to resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_12 (InputLayer)            (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 32, 32, 3)     228         input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 32, 32, 3)     228         conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 32, 32, 3)     228         conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 32, 32, 3)     0           conv2d_23[0][0]                  \n",
      "                                                                   input_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 684\n",
      "Trainable params: 684\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# When the layers being added are the same shape\n",
    "from keras import layers\n",
    "\n",
    "# We assume the existance of a 4D input tensor 'x'\n",
    "x = Input(shape=(32, 32, 3))\n",
    "\n",
    "# We apply some transformation to x\n",
    "y = layers.Conv2D(3, 5, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(3, 5, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(3, 5, activation='relu', padding='same')(y)\n",
    "\n",
    "# We add the original x back to the output features\n",
    "y = layers.add([y, x])\n",
    "\n",
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_22 (InputLayer)            (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)               (None, 28, 28, 3)     228         input_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)               (None, 24, 24, 3)     228         conv2d_59[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)               (None, 22, 22, 3)     84          conv2d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D)  (None, 11, 11, 3)     0           conv2d_61[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)               (None, 11, 11, 3)     12          input_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 11, 11, 3)     0           max_pooling2d_13[0][0]           \n",
      "                                                                   conv2d_62[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 552\n",
      "Trainable params: 552\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# When the layers being added are of different sizes\n",
    "x = Input(shape=(32, 32, 3))\n",
    "\n",
    "y = layers.Conv2D(3, 5, activation='relu')(x)  # y.shape = 28, 28, 3\n",
    "y = layers.Conv2D(3, 5, activation='relu')(y)  # y.shape = 24, 24, 3\n",
    "y = layers.Conv2D(3, 3, activation='relu')(y)  # y.shape = 22, 22, 3\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)       # y.shape = 11, 11, 3\n",
    "\n",
    "# Downsample x using a 1x1 convolution\n",
    "residual = layers.Conv2D(3, 1, strides=3)(x)   # residual.shape = 11, 11, 3\n",
    "\n",
    "y = layers.add([y, residual])\n",
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Siamese Networks\n",
    "\n",
    "The main property of these networks is that different layers share the same weights. These networks start out with two different branches and then merge later on. To look at they look similar to the mult-input models. However, the fundamental difference is that both branches share the same weights. When they are being trained, the gradients coming down both branches are added up before being subtracted from the common weights so that both the branches continue to have the same weights throughout the training.\n",
    "\n",
    "## Motivating Examples\n",
    "The main application is to learn some sort of a similarity metric (or a discriminative metric) that can distinguish between two similar but different samples. Another defining characteristic is that the samples within the pair are interchangeable, i.e., if I run sample1 on branch1 and sample2 on branch2 and get two vector representations - vector1 and vector2 which are then compared and found to be same (or different), then if I swap the samples and provide sample1 to branch2 and sample2 to branch1, it will produce vectors vector2 and vector1 which will found to have the same distance (or any other similarity metric).\n",
    "\n",
    "### Face Verification\n",
    "A Siamese network that learns to distinguish between images of people, when the two images are of the same person, the network outputs a small number, when the two images are of different people, the network outputs a big number. A concrete example is to use face scans at turnstiles instead of ID cards to allow access to a building. One way to speed up inference is to store the vector representation of all known people who are allowed access in a database. Then when a person scans their face, only the branch of the network is deployed which outputs the vector representation of this person's face. This vector representation is compared with all other vector reprsentation and the closest one (within a certain threshold) is chosen as the \"owner\" of that face.\n",
    "\n",
    "### One-Shot Image Recognition\n",
    "A Siamese network that is first trained on a few images, some of the same object, some of different objects. Once the network has learnt the similarity metric, it can then be given images that belong to completely different classes from anything seen in the training set, and can still discriminate between images of the same object vs. images of different objects.\n",
    "\n",
    "### Signature Verfication\n",
    "A Siamese network that is trained on some pairs of signatures that may or may not belong to the same person. The branches learn a vector reprsentation (a.k.a the metric) and the \"head\" or the end layers discriminate between the two signatures. \n",
    "\n",
    "### Semantic Similarity\n",
    "A model that attempts to assess the semantic similarity between two sentances. The model would have two inputs (the two sentances to compare) and would output a score between 0 and 1 depending on whether they  are different or similar. This could be used for deduping natural language queries in a dialog system. As usual, the two sentances are interchangeable, because semantic similarity is a symmetrical relationship: the similarity of A to B is identical to the similarity between B to A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers, Input\n",
    "from keras.models import Model\n",
    "\n",
    "# Instantiate a single LSTM layer\n",
    "lstm = layers.LSTM(32)\n",
    "\n",
    "# Build the left branch of the model - inputs are variable length sequences of vectors of size 128\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "# Buildng the right branch of the model\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model([left_input, right_input], predictions)\n",
    "\n",
    "# When this model is learnt, the weights of the lstm layers are updated based on both inputs\n",
    "model.fit([left_data, right_data], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In addition to having common layers, entire models, i.e., `Model` objects themselves can also act as \"layer\" and can be used in a Siamese network. Here is another motivating example.\n",
    "\n",
    "A vision model that uses a dual camera as its inputs: two parallel camersa, a few centimeters apart from each other. Such a model could be capable of preceiving depth, which can be useful in many applications. You shouldn't need two independant models for extracting visual features from the two cameras before merging the feeds. Such low-leve processing can be shared across the two inputs, i.e., done via layers that use the same weights and thus share the same representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers, applications, Input\n",
    "\n",
    "# Our base image processing model will be the Xception network\n",
    "# But only its convolution base, not its final  classification layer.\n",
    "xception_base = applications.Xception(weights=None, include_top=False)\n",
    "\n",
    "# Inputs are 250x250 RGB images\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "\n",
    "# We call the same vision model twice\n",
    "left_features = xception_base(left_input)\n",
    "right_features = xception_base(right_input)\n",
    "\n",
    "# The merged features contain information from both feeds\n",
    "merged_features = layers.concatenate([left_features, right_features], axis=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
