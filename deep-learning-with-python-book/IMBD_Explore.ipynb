{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset\n",
    "\n",
    "This dataset consists of 50,000 movie reviews, with a train/test split of 50% each. Both the input features and target are simply a 1D list of numpy arrays. Each element of the input array is a document vector, which in turn is represented as a Python list of integers. Each element in the target is a single number - either 0 or 1.\n",
    "\n",
    "When Keras is creating the vocabulary of the dataset, it will assign indexes to tokens corresponding to their frequencies. So the token with the highest frequency has an index of 1, the next most frequent token has an index of 2, and so on. When we load the data we specify the `num_words` argument. This says only the top `num_words` words should be part of the vocabulary. A simple way to implement this is to throw away all tokens whose indexes are greater than `num_words`. A point of note here is that indexes start with 1, no token is assigned index 0.\n",
    "\n",
    "The document vector consists of the indexes of the word at that position. Words that are not in the vocabulary are assigned a special *unknown* token. Given each review is of different length, it follows that lists in the input array are also of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "<class 'numpy.ndarray'> <class 'list'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n",
      "0 1\n",
      "218 189 141\n",
      "72 646 466\n",
      "1 9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('br', 7),\n",
       " ('in', 8),\n",
       " ('it', 9),\n",
       " ('i', 10),\n",
       " ('this', 11),\n",
       " ('that', 12),\n",
       " ('was', 13),\n",
       " ('as', 14),\n",
       " ('for', 15)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 25,000 samples in the train and test dataset each\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# Input is an array of lists\n",
    "print(type(x_train), type(x_train[0]))\n",
    "\n",
    "# Target is an array of ints with values 0 or 1\n",
    "print(type(y_train), type(y_train[0]))\n",
    "print(np.min(y_train), np.max(y_train))\n",
    "\n",
    "# Each review has a different length\n",
    "print(len(x_train[0]), len(x_train[1]), len(x_train[2]))\n",
    "print(len(x_test[0]), len(x_test[1]), len(x_test[2]))\n",
    "\n",
    "# Only the top 10,000 (actually 9,999) words are included\n",
    "min_index = min([min(vec) for vec in x_train])\n",
    "max_index = max([max(vec) for vec in x_train])\n",
    "print(min_index, max_index)\n",
    "\n",
    "# Indexes start with 1\n",
    "sorted(imdb.get_word_index().items(), key=lambda kv: kv[1])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weirdnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weirdness 1\n",
    "The indexes given by `imbd.get_word_index()` do not actually correspond with the indexes that appear in the word vector. They are all offset by 3. To distinguish these two types of indexes, lets call the indexes given by `get_word_index()` as raw indexes and the actual indexes as indexes. So a word with raw index *i* will actually appear in the document with index *i+3*. Say the document has the word *this*. It will not appear as number *11* in the document vector. Instead it will appear as number *14*. Conversly, if I see the number *11* in the document, that is its actual index, its raw index will be *8*, which means it represents the word *in*. In other words - `index = raw_index + 3`. \n",
    "\n",
    "Given the offset described above, the actual indexes should start from *4* because the raw indexes start from *1*. However, we saw above that the actual indexes start from *1*. This is because indexes *1*, *2*, and *3* have special meanings.\n",
    "\n",
    "In the cells below I convert a document vector back to a document. First I use the raw index for the conversion, but the resulting document does not make a lot of sense. Then I use the actual indexes for the conversion and the resulting document is resonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "# A document with the raw indexes does not make much sense\n",
    "raw_indexes = imdb.get_word_index()\n",
    "raw_tokens = {raw_index: word for word, raw_index in raw_indexes.items()}\n",
    "vec = x_train[0]\n",
    "doc = ' '.join([raw_tokens[index] for index in vec])\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<special> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <special> is an amazing actor and now the same being director <special> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <special> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <special> to the two little boy's that played the <special> of norman and paul they were just brilliant children are often left out of the <special> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# A document with actual indexes is reasonable\n",
    "tokens = {raw_index + 3: word for word, raw_index in raw_indexes.items()}\n",
    "# Adding the speical tokens\n",
    "tokens[1] = '<special>'\n",
    "tokens[2] = '<special>'\n",
    "tokens[3] = '<special>'\n",
    "vec = x_train[0]\n",
    "tokens_in_doc = [tokens[index] for index in vec] \n",
    "doc = ' '.join(tokens_in_doc)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weirdness 2\n",
    "Indexes 1, 2, and 3 should have had special meanings. However, only indexes 1 and 2 have special meanings. 1 means *start of document* and 2 means *unknown token*. However, index number 3 has been completely forgotten! It has no token assigned to it and it does not appear in the dataset at all! When the proper args are given to load_data, I might see index 0. This has the special meaning of *padding*.\n",
    "\n",
    "In the cell below first I re-print the first review with the right special tokens. Then I prove that there is no index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <unknown> is an amazing actor and now the same being director <unknown> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <unknown> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <unknown> to the two little boy's that played the <unknown> of norman and paul they were just brilliant children are often left out of the <unknown> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# Printing the review with the right special tokens\n",
    "tokens[0] = '<padding>'\n",
    "tokens[1] = '<start>'\n",
    "tokens[2] = '<unknown>'\n",
    "tokens[3] = '<?>'\n",
    "tokens_in_doc = [tokens[index] for index in vec] \n",
    "doc = ' '.join(tokens_in_doc)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Proof that index 3 does not appear anywhere in the data\n",
    "for i, seq in enumerate(x_train):\n",
    "    if 3 in seq:\n",
    "        print(f'found it in {i}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weirdness 3\n",
    "Instead of simply dropping tokens that are not part of the vocabulary, they are replaced with the special *unknown* token. In the code below, notice that *redford's* has an index of 22,665 (raw index of 22,662), so in x_train it was dropped out of the vocabulary and its occurance replaced with the special *unknown* token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[458] = direction\t[458] = direction\n",
      "[4468] = everyone's\t[4468] = everyone's\n",
      "[66] = really\t[66] = really\n",
      "[3941] = suited\t[3941] = suited\n",
      "[4] = the\t[4] = the\n",
      "[173] = part\t[173] = part\n",
      "[36] = they\t[36] = they\n",
      "[256] = played\t[256] = played\n",
      "[5] = and\t[5] = and\n",
      "[25] = you\t[25] = you\n",
      "[100] = could\t[100] = could\n",
      "[43] = just\t[43] = just\n",
      "[838] = imagine\t[838] = imagine\n",
      "[112] = being\t[112] = being\n",
      "[50] = there\t[50] = there\n",
      "[670] = robert\t[670] = robert\n",
      "[2] = <unknown>\t[22665] = redford's\n",
      "[9] = is\t[9] = is\n",
      "[35] = an\t[35] = an\n",
      "[480] = amazing\t[480] = amazing\n",
      "22662 redford's\n"
     ]
    }
   ],
   "source": [
    "# Load the data without dropping any tokens\n",
    "(x_train_full, _), (_, _) = imdb.load_data()\n",
    "vec_full = x_train_full[0]\n",
    "tokens_in_doc_full = [tokens[index] for index in vec_full] \n",
    "print('\\n')\n",
    "for index, token, index_full, token_full in zip(vec[10:30], tokens_in_doc[10:30], vec_full[10:30], tokens_in_doc_full[10:30]):\n",
    "    print(f'[{index}] = {token}\\t[{index_full}] = {token_full}')\n",
    "    \n",
    "print(raw_indexes[\"redford's\"], tokens[raw_indexes[\"redford's\"] + 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
