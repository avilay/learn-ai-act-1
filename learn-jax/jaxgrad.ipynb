{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, lax, vmap, pmap, grad, value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "The `grad` transform returns a function that can compute the gradient of the passed in function at any point. The original function should be a scalar-valued function, i.e., it can take in multi-dimensional tensors as input, but should output a scalar.\n",
    "\n",
    "By default, if $f$ is a function of multiple arguments, `grad` will calculate the gradient w.r.t the first arg -\n",
    "$$\n",
    "f(x, y) \\\\\n",
    "grad(f) = \\frac{\\partial f}{\\partial x}\n",
    "$$\n",
    "\n",
    "But I can change this and specify the index of the arugments I want to calculate the gradients w.r.t.\n",
    "\n",
    "Applying `grad` multiple times, gives me the second- third- and so on order derivatives.\n",
    "\n",
    "$$\n",
    "grad(grad(f)) = \\frac{\\partial^2 f}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "Of course this will only work if the gradient itself is a scalar. E.g., consider this vector input scalar output function -\n",
    "\n",
    "$$\n",
    "f(\\mathbf w, b, \\mathbf x) = \\mathbf w^T \\mathbf x + b \\\\\n",
    "\\frac{\\partial f}{\\partial \\mathbf w} = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial w_1} \\\\\n",
    "\\frac{\\partial f}{\\partial w_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying `grad` a second time will not work in this case. To calculate the Hessians I'll have to use `jacfwd` or `jacrev` which computes the gradient of vector valued function. Mathematically both do the same thing, but the implementation is different based on autograd algorithms, which I have now forgotten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(w, b, x):\n",
    "    return w.T @ x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_ = grad(perceptron, argnums=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = jnp.array([1., 2.])\n",
    "b = 0.5\n",
    "x = jnp.array([1.5, 2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(7., dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron(w, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5 2.5]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dw, db = perceptron_(w, b, x)\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful function for debugging is the `value_and_grad` function that returns a function that returns not just the gradient, but also the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_and_perceptron_ = value_and_grad(perceptron, argnums=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(7., dtype=float32),\n",
       " (Array([1.5, 2.5], dtype=float32), Array(1., dtype=float32, weak_type=True)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_and_perceptron_(w, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n",
      "Gradient only defined for scalar-output functions. Output had shape: (2,).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    grad(grad(perceptron))(w, b, x)\n",
    "except Exception as err:\n",
    "    print(f\"{type(err)}\\n{err}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In typical DL scenarios, the loss function is usually differentiated. However, instead of just returning the loss value, it is convenient for the loss function to also return the result of the forward pass, e.g., the logits calculated. This will help in calculating training metrics. To enable this, use the `aux=True` flag for both `grad` and `value_and_grad` functions.\n",
    "\n",
    "In the example below, the `grad(loss)()` function will return the gradient as its first return value, but the second return value (the auxilliary value) will be just a normal computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    val = x**2\n",
    "    aux = [x**3, x**4]\n",
    "    return val, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 [Array(8., dtype=float32, weak_type=True), Array(16., dtype=float32, weak_type=True)]\n"
     ]
    }
   ],
   "source": [
    "dx, aux = grad(f, has_aux=True)(2.)\n",
    "print(dx, aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "[Array(27., dtype=float32, weak_type=True), Array(81., dtype=float32, weak_type=True)]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "(val, aux), dx = value_and_grad(f, has_aux=True)(3.)\n",
    "print(val)\n",
    "print(aux)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c957e4ca480fc31da01b2648e097b4e55db834b6351128636991f182c884d81e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
