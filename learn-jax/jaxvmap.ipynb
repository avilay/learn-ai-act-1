{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, lax, vmap, pmap, grad, value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Vectorization\n",
    "Define the op for a \"single\" instance, and then use `vmap` to call it with batches of input. `vmap` is usually an order of magnitude faster than naive for-loop through the batch. It seems to me that automatic vectorization will use threads/warps on a single accelerator to do DDP with the input. It is not doing any clever vector math.\n",
    "\n",
    "Update: I recdently saw a [Twitter thread](https://twitter.com/jakevdp/status/1612544608646606849?s=20&t=s8uBb3teX19T3ELQ07melA) that seemed to imply that `vmap` does indeed do some clever linalg. I can also verify this by examining the jaxpr of the vmapped function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jnp.dot` is defined for two 1D vectors of the same size. It will not work with 2D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(89.16817, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jax.random.normal(key, shape=(100,), dtype=jnp.float32)\n",
    "y = jax.random.normal(key, shape=(100,), dtype=jnp.float32)\n",
    "jnp.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n",
      "Incompatible shapes for dot: got (5, 100) and (5, 100).\n"
     ]
    }
   ],
   "source": [
    "batch_x = jax.random.normal(key, shape=(5,100), dtype=jnp.float32)\n",
    "batch_y = jax.random.normal(key, shape=(5,100), dtype=jnp.float32)\n",
    "try:\n",
    "    jnp.dot(batch_x, batch_y)\n",
    "except Exception as err:\n",
    "    print(f\"{type(err)}\\n{err}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I can `vmap` it so that it will work with one extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([103.42397 , 116.75762 ,  97.10165 ,  85.551155,  93.10307 ],      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdot = vmap(jnp.dot)\n",
    "vdot(batch_x, batch_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is way faster than a naive for-loop implementation. The naive for-loop implementation cannot be jitted because its calculation depends on the size of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vdot(batch_x, batch_y):\n",
    "    m = len(batch_x)\n",
    "    dots = []\n",
    "    for idx in range(m):\n",
    "        dot = jnp.dot(batch_x[idx], batch_y[idx])\n",
    "        dots.append(dot)\n",
    "    return jnp.array(dots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 ms ± 37 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit naive_vdot(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 µs ± 19.5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare it with a non-jitted vmapped function\n",
    "%timeit vdot(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 µs ± 34.1 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit(vdot)(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[5,100]\u001b[39m b\u001b[35m:f32[5,100]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[5]\u001b[39m = dot_general[\n",
       "      dimension_numbers=(((1,), (1,)), ((0,), (0,)))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] a b\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(vdot)(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[100]\u001b[39m b\u001b[35m:f32[100]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[]\u001b[39m = dot_general[\n",
       "      dimension_numbers=(((0,), (0,)), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] a b\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(jnp.dot)(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `vmap` assumes that the first dimension of the input tensors is the batch dimensions. We can also explicitly specify this using the `in_axes` argument. This is useful when some of the inputs are batched and others are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 4.997196  ,  0.39028692, -0.4550171 ,  5.45161   , -1.3241603 ],      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdot2 = vmap(jnp.dot, in_axes=(None, 0))\n",
    "vdot2(x, batch_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me define a simple perceptron with this -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine(W, b, x):\n",
    "    return W @ x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-9.891567  ,  3.198782  , -5.058924  ,  5.849048  , -1.042989  ,\n",
       "        6.441183  , -3.0672126 , -2.5102026 , -0.10067141,  0.2673887 ],      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input has 20 features and output has a dim of 10 (classes?)\n",
    "W = jax.random.normal(key, shape=(10, 20), dtype=jnp.float32)\n",
    "b = jax.random.normal(key, shape=(10,), dtype=jnp.float32)\n",
    "x = jax.random.normal(key, shape=(20,), dtype=jnp.float32)\n",
    "affine(W, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n",
      "dot_general requires contracting dimensions to have the same shape, got (20,) and (3,).\n"
     ]
    }
   ],
   "source": [
    "# now with a batch size of 3\n",
    "batch_x = jax.random.normal(key, shape=(3, 20), dtype=jnp.float32)\n",
    "try:\n",
    "    affine(W, b, batch_x)\n",
    "except Exception as err:\n",
    "    print(f\"{type(err)}\\n{err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 2.4496658 , -6.5097513 ,  7.7877793 , -2.5308816 ,  4.5156784 ,\n",
       "         8.680212  , -0.36133832, -8.290409  ,  5.073617  ,  6.4655223 ],\n",
       "       [-1.755568  , -0.21376276,  2.8784447 ,  0.7835917 , -7.0984244 ,\n",
       "        -4.86695   , -4.7300005 , -8.867712  ,  2.7317257 , -0.13192517],\n",
       "       [-3.3044035 ,  2.625197  , -1.6779847 ,  2.484176  , -3.8615348 ,\n",
       "         8.243641  ,  0.31611758,  6.9084396 , -0.9615234 , -4.7001905 ]],      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(affine, in_axes=(None, None, 0))(W, b, batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c957e4ca480fc31da01b2648e097b4e55db834b6351128636991f182c884d81e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
