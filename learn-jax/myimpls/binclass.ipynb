{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 0], dtype=uint32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Batch = namedtuple(\"Batch\", [\"X\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_classes=2,\n",
    "    n_samples=1_000_000,\n",
    "    random_state=0,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=7,\n",
    "    n_repeated=3,\n",
    "    flip_y=0.05,\n",
    "    class_sep=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._X[idx], self._y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._X.shape[0]\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    xs, ys = zip(*samples)\n",
    "    X = jnp.vstack(jnp.array(x) for x in xs)\n",
    "    y = jnp.array([y for y in ys])\n",
    "    return Batch(X, y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)\n",
    "trainset = BinDataset(X_train, y_train)\n",
    "valset = BinDataset(X_val, y_val)\n",
    "print(len(trainset), len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params, X):\n",
    "    for param in params[:-1]:\n",
    "        W, b = param[\"W\"], param[\"b\"]\n",
    "        X = jax.nn.relu(X @ W + b)\n",
    "    W, b = params[-1][\"W\"], params[-1][\"b\"]\n",
    "    # p = jax.nn.sigmoid(X @ W.T + b)\n",
    "    logits = X @ W + b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(key, in_features, out_features):\n",
    "    W = jax.nn.initializers.glorot_normal()(key, (in_features, out_features))\n",
    "    var = jnp.sqrt(out_features)\n",
    "    b = jax.random.uniform(key, (out_features,), minval=-var, maxval=var)\n",
    "    if out_features == 1:\n",
    "        W = W.squeeze()\n",
    "        b = b.squeeze()\n",
    "    return {\"W\": W, \"b\": b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(params, batch):\n",
    "    X, y = batch.X, batch.y\n",
    "    logits = model(params, X)\n",
    "    bce = optax.sigmoid_binary_cross_entropy(logits, y)\n",
    "    mean_bce = jnp.mean(bce)\n",
    "    return mean_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"learning_rate\": 0.006, \"n_epochs\": 4, \"batch_size\": 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "params = [init_linear(key, 20, 8), init_linear(key, 8, 1)]\n",
    "traindl = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=hyperparams[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "valdl = DataLoader(\n",
    "    valset,batch_size=10000, shuffle=False, drop_last=False, collate_fn=collate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def step(params, opt_state, batch):\n",
    "    loss_val, grads = jax.value_and_grad(loss)(params, batch)\n",
    "    updates, opt_state = optim.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 0: Val Loss = 0.26663   Train Loss = 0.66109\n",
      "Epoch 0 Step 10000: Val Loss = 0.25336   Train Loss = 0.25450\n",
      "Epoch 0 Step 20000: Val Loss = 0.24539   Train Loss = 0.24942\n",
      "Epoch 1 Step 0: Val Loss = 0.25001   Train Loss = 0.25489\n",
      "Epoch 1 Step 10000: Val Loss = 0.25848   Train Loss = 0.25245\n",
      "Epoch 1 Step 20000: Val Loss = 0.25031   Train Loss = 0.25255\n"
     ]
    }
   ],
   "source": [
    "# optim = optax.adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "optim = optax.adam(learning_rate=0.0006)\n",
    "opt_state = optim.init(params)\n",
    "train_losses = []\n",
    "# for epoch in range(hyperparams[\"n_epochs\"]):\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(traindl):\n",
    "        params, opt_state, train_loss = step(params, opt_state, batch)\n",
    "        train_losses.append(train_loss)\n",
    "        if i % 10000 == 0:\n",
    "            val_losses = []\n",
    "            for valbatch in valdl:\n",
    "                val_loss = loss(params, valbatch)\n",
    "                val_losses.append(val_loss)\n",
    "            val_loss = jnp.mean(jnp.array(val_losses))\n",
    "            train_loss = jnp.mean(jnp.array(train_losses))\n",
    "            train_losses = []\n",
    "            print(f\"Epoch {epoch} Step {i}: Val Loss = {val_loss:.5f}   Train Loss = {train_loss:.5f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 0 Step 0: Val Loss = 2.38983   Train Loss = 2.51969\n",
    "Epoch 0 Step 10000: Val Loss = 0.35811   Train Loss = 0.39189\n",
    "Epoch 0 Step 20000: Val Loss = 0.33252   Train Loss = 0.34015\n",
    "Epoch 0 Step 30000: Val Loss = 0.31942   Train Loss = 0.32632\n",
    "Epoch 0 Step 40000: Val Loss = 0.31470   Train Loss = 0.32119\n",
    "Epoch 1 Step 0: Val Loss = 0.31439   Train Loss = 0.31875\n",
    "Epoch 1 Step 10000: Val Loss = 0.31506   Train Loss = 0.31685\n",
    "Epoch 1 Step 20000: Val Loss = 0.31275   Train Loss = 0.31332\n",
    "Epoch 1 Step 30000: Val Loss = 0.30700   Train Loss = 0.31184\n",
    "Epoch 1 Step 40000: Val Loss = 0.30697   Train Loss = 0.30810\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c957e4ca480fc31da01b2648e097b4e55db834b6351128636991f182c884d81e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
