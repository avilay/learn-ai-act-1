{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "import os.path as path\n",
    "\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "DATAROOT = path.expanduser(\"~/mldata/pytorch\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xform = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 48000, Validation set size: 12000\n"
     ]
    }
   ],
   "source": [
    "datapath = path.join(DATAROOT, \"fashion-mnist\")\n",
    "train_val_set = tv.datasets.FashionMNIST(datapath, download=True, train=True, transform=xform)\n",
    "train_size = int(len(train_val_set) * 0.8)\n",
    "val_size = len(train_val_set) - train_size\n",
    "trainset, valset = t.utils.data.random_split(train_val_set, [train_size, val_size])\n",
    "print(f\"Training set size: {train_size}, Validation set size: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "testset = tv.datasets.FashionMNIST(datapath, download=True, train=False, transform=xform)\n",
    "print(f\"Test set size: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = t.nn.Sequential(OrderedDict([\n",
    "        (\"flatten\", t.nn.Flatten()),\n",
    "        (\"fc1\", t.nn.Linear(784, 128)),\n",
    "        (\"relu1\", t.nn.ReLU()),\n",
    "        (\"fc2\", t.nn.Linear(128, 64)),\n",
    "        (\"relu2\", t.nn.ReLU()),\n",
    "        (\"fc3\", t.nn.Linear(64, 32)),\n",
    "        (\"relu3\", t.nn.ReLU()),\n",
    "        (\"logits\", t.nn.Linear(32, 10))\n",
    "    ]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    assert outputs.shape[0] == targets.shape[0]\n",
    "    predictions = t.argmax(outputs, dim=1)\n",
    "    correct = t.sum(predictions == targets).item()\n",
    "    return correct / targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparams:\n",
    "    batch_size: int = 10\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 0.0001\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"learning_rate\": self.learning_rate\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image, _ = next(iter(trainset))\n",
    "batch_of_one = t.unsqueeze(image, 0)\n",
    "print(image.shape, batch_of_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0990, -0.0407,  0.0240,  0.0867,  0.0274, -0.0667, -0.1145,  0.1009,\n",
       "          0.0656,  0.1739]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model().forward(batch_of_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 28, 28]) torch.Size([5000]) torch.Size([5000, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = t.utils.data.DataLoader(valset, batch_size=5000)\n",
    "images, targets = next(iter(dl))\n",
    "outputs = create_model().forward(images)\n",
    "print(images.shape, targets.shape, outputs.shape)\n",
    "targets = targets.detach()\n",
    "ouputs = outputs.detach()\n",
    "accuracy(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, loss_fn, epochs, trainloader, valloader):\n",
    "    model = model.to(DEVICE)\n",
    "    for epoch in range(epochs):\n",
    "        # Process the training set\n",
    "        train_losses = []\n",
    "        train_outputs = t.empty(0, 10)\n",
    "        train_targets = t.tensor([], dtype=t.long)\n",
    "        model.train()\n",
    "        with t.enable_grad():\n",
    "            for images, targets in trainloader:\n",
    "                images = images.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                outputs = model.forward(images)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                train_losses.append(loss.detach())\n",
    "                train_outputs = t.cat((train_outputs, outputs.detach()))\n",
    "                train_targets = t.cat((train_targets, targets.detach()))\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_acc = accuracy(train_outputs, train_targets)\n",
    "\n",
    "        # Calculate the validation metrics\n",
    "        val_losses = []\n",
    "        val_outputs = t.empty(0, 10)\n",
    "        val_targets = t.tensor([], dtype=t.long)\n",
    "        model.eval()\n",
    "        with t.no_grad():\n",
    "            for images, targets in valloader:\n",
    "                images = images.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_losses.append(loss.detach())\n",
    "                val_outputs = t.cat((val_outputs, outputs.detach()))\n",
    "                val_targets = t.cat((val_targets, targets.detach()))\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = accuracy(val_outputs, val_targets)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}:\")\n",
    "        print(f\"Loss: train={train_loss:.3f}, validation={val_loss:.3f}\")\n",
    "        print(f\"Accuracy: train={train_acc:.2f}, validation={val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams = Hyperparams(batch_size=32, epochs=10, learning_rate=0.05)\n",
    "hparams = Hyperparams(batch_size=32, epochs=13, learning_rate=0.25)\n",
    "model = create_model()\n",
    "optim = t.optim.SGD(model.parameters(), lr=hparams.learning_rate)\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=hparams.batch_size, shuffle=True)\n",
    "valloader = t.utils.data.DataLoader(valset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "Loss: train=0.655, validation=0.492\n",
      "Accuracy: train=0.76, validation=0.83\n",
      "\n",
      "Epoch 1:\n",
      "Loss: train=0.448, validation=0.399\n",
      "Accuracy: train=0.84, validation=0.86\n",
      "\n",
      "Epoch 2:\n",
      "Loss: train=0.409, validation=0.373\n",
      "Accuracy: train=0.85, validation=0.87\n",
      "\n",
      "Epoch 3:\n",
      "Loss: train=0.373, validation=0.408\n",
      "Accuracy: train=0.86, validation=0.85\n",
      "\n",
      "Epoch 4:\n",
      "Loss: train=0.352, validation=0.353\n",
      "Accuracy: train=0.87, validation=0.87\n",
      "\n",
      "Epoch 5:\n",
      "Loss: train=0.337, validation=0.360\n",
      "Accuracy: train=0.88, validation=0.87\n",
      "\n",
      "Epoch 6:\n",
      "Loss: train=0.319, validation=0.347\n",
      "Accuracy: train=0.88, validation=0.88\n",
      "\n",
      "Epoch 7:\n",
      "Loss: train=0.313, validation=0.386\n",
      "Accuracy: train=0.89, validation=0.87\n",
      "\n",
      "Epoch 8:\n",
      "Loss: train=0.300, validation=0.335\n",
      "Accuracy: train=0.89, validation=0.89\n",
      "\n",
      "Epoch 9:\n",
      "Loss: train=0.297, validation=0.347\n",
      "Accuracy: train=0.89, validation=0.88\n",
      "\n",
      "Epoch 10:\n",
      "Loss: train=0.284, validation=0.364\n",
      "Accuracy: train=0.90, validation=0.88\n",
      "\n",
      "Epoch 11:\n",
      "Loss: train=0.280, validation=0.344\n",
      "Accuracy: train=0.90, validation=0.88\n",
      "\n",
      "Epoch 12:\n",
      "Loss: train=0.271, validation=0.327\n",
      "Accuracy: train=0.90, validation=0.88\n"
     ]
    }
   ],
   "source": [
    "train(model, optim, loss_fn, hparams.epochs, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "testloader = t.utils.data.DataLoader(testset, batch_size=len(testset))\n",
    "images, targets = next(iter(testloader))\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    outputs = model(images)\n",
    "    test_acc = accuracy(outputs.detach(), targets.detach())\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
