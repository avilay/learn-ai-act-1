{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say $z$ is a function of $\\mathbf x$ and $\\mathbf w$ that outputs a scalar value. This is the most common setup in ML loss functions.\n",
    "\n",
    "$$\n",
    "z = 3 \\mathbf x^T (\\mathbf w + 2)^{\\circ 2} \\\\\n",
    "$$\n",
    "\n",
    "If $\\mathbf x$ is a 3-element vector -\n",
    "$$\n",
    "\\mathbf x = \\begin{bmatrix}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "x_3\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "and correspondingly $\\mathbf w$ is also a 3-element vector -\n",
    "$$\n",
    "\\mathbf w = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "Then $z$ can be written as -\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= 3 \\cdot \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "(w_1 + 2)^2 \\\\\n",
    "(w_2 + 2)^2 \\\\\n",
    "(w_3 + 2)^2 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= 3x_1(w_1 + 2)^2 + 3x_2(w_2 + 2)^2 + 3x_3(w_3 + 2)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And the Jacobian of $z$ is a row vector - \n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf w} z &= \\begin{bmatrix} \\frac{\\partial z}{\\partial w_1} & \\frac{\\partial z}{\\partial w_2} & \\frac{\\partial z}{\\partial w_2} \\\\ \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} 6x_1(w_1 + 2) & 6x_2(w_2 + 2) & 6x_3(w_3 + 2) \\\\ \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This Jacobian is also colloquially known as the gradient of $\\mathbf w$. Keeping $\\mathbf x$ constant at $\\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$ the gradient becomes -\n",
    "$$\n",
    "\\nabla_{\\mathbf w} z = \\begin{bmatrix} 3(w_1 + 2) & 3(w_2 + 2) & 3(w_3 + 2) \\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now for different values of $\\mathbf w$, we will have different values of the gradient. E.g., when $\\mathbf w = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $z$ evaluates to -\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= 3 \\times 0.5 \\times (1 + 2)^2 + 3 \\times 0.5 \\times (1 + 2)^2 + 3 \\times 0.5 \\times (1 + 2)^2 \\\\\n",
    "&= 13.5 + 13.5 + 13.5 \\\\\n",
    "&= 40.5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And the gradient of $\\mathbf w$ becomes -\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf w} z &= \\begin{bmatrix} 3(1 + 2) & 3(1 + 2) & 3(1 + 2) \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} 9 & 9 & 9 \\\\ \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the `requires_grad` flag on `w` we are telling PyTorch that this is the variable that we'll differentiate with respect to and will have a gradient. `x` will not have a graident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.5000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.full((3,), 0.5)\n",
    "w = t.ones(3, requires_grad=True)\n",
    "w2 = (w + 2) ** 2\n",
    "z = 3 * t.dot(x, w2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `Tensor.backward()` will run the backpropagation on this compute graph which will calculate the gradients for the leaves of this graph and store it alongside the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I try to backprop through this compute graph again it'll fail. I need to recalculate the grpah with a new $\\mathbf w$ and then I can run `backward` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(72., grad_fn=<MulBackward0>)\n",
      "grad w =  tensor([12., 12., 12.])\n"
     ]
    }
   ],
   "source": [
    "w = t.full((3,), 2., requires_grad=True)\n",
    "w2 = (w + 2) ** 2\n",
    "z = 3 * t.dot(x, w2)\n",
    "print(\"z = \", z)\n",
    "z.backward()\n",
    "print(\"grad w = \", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason I do need to backprop multiple times, then it will accumulate the gradients on the leaf nodes. And the way I can circumvent the above error is by setting the `retain_graph` flag. Normally this is never needed and according to the [documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch-tensor-backward) I should avoid setting this flag. But for demonstrating this concept I'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(40.5000, grad_fn=<MulBackward0>)\n",
      "grad w =  tensor([9., 9., 9.])\n",
      "grad w =  tensor([18., 18., 18.])\n"
     ]
    }
   ],
   "source": [
    "w = t.ones((3,), requires_grad=True)\n",
    "w2 = (w + 2) ** 2\n",
    "z = 3 * t.dot(x, w2)\n",
    "print(\"z = \", z)\n",
    "z.backward(retain_graph=True)\n",
    "print(\"grad w = \", w.grad)\n",
    "z.backward(retain_graph=True)\n",
    "print(\"grad w = \", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is just another tensor that can be changed to anything I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad w so far =  tensor([18., 18., 18.])\n",
      "grad w after I manually changed it =  tensor([1., 2., 3.])\n",
      "grad w after backprop =  tensor([10., 11., 12.])\n"
     ]
    }
   ],
   "source": [
    "print(\"grad w so far = \", w.grad)\n",
    "w.grad.data = t.tensor([1., 2., 3.])\n",
    "print(\"grad w after I manually changed it = \", w.grad)\n",
    "z.backward(retain_graph=True)\n",
    "print(\"grad w after backprop = \", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the example above, when the output is a vector the call to backprop needs the initial accumulated gradient to start from. I think this is because the implementation is the same when starting the backprop from somewhere in the middle of the compute graph. It needs some gradients to flow in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z = 3(x+2)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = 6(x+2)\n",
    "$$\n",
    "\n",
    "$$x = 0, \\quad \\frac{dz}{dx} = 12$$\n",
    "\n",
    "$$x = 1, \\quad \\frac{dz}{dx} = 18$$\n",
    "\n",
    "$$x = 2, \\quad \\frac{dz}{dx} = 24$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "z =  tensor([27., 27., 27., 27.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# x = torch.zeros(4, requires_grad=True)\n",
    "x = torch.ones(4, requires_grad=True)\n",
    "# x = torch.tensor([2.0, 2.0, 2.0, 2.0], requires_grad=True)\n",
    "y = x + 2\n",
    "y2 = y**2\n",
    "z = 3*y2\n",
    "print(\"x = \", x)\n",
    "print(\"z = \", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dz = torch.ones_like(z)\n",
    "z.backward(dz_dz, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([18., 18., 18., 18.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 36., 36., 36.])\n",
      "tensor([54., 54., 54., 54.])\n"
     ]
    }
   ],
   "source": [
    "# Everytime I back propagate, the gradients are accumulated\n",
    "# i.e., dz_dx := dz_dx(old) + dz_dx(new)\n",
    "z.backward(dz_dz, retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "z.backward(dz_dz, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The grads themselves are fully mutable, so I can reset their values to 0 (say)\n",
    "x.grad = torch.zeros_like(x)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18., 18., 18., 18.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(dz_dz, retain_graph=True)\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "784114715d987f376fd768172d6fb64b1fb77afe12a7e2b607779dcdec22a151"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
