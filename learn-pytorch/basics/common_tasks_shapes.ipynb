{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes Used in Common Tasks\n",
    "I am always confused about what shape to output and what loss to use for common tasks like binary classification, multiclass classification, and regression. This notebook provides recipes for these common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "Most datasets will have a single scalar, either $0$ or $1$, as the target variable. This means that after being batched by the data loader, the targets will have shape `torch.Size(batch_size)`, i.e., a single \"row\" vector. For this reason my usual way is to ensure that the model also outputs a row vector.\n",
    "\n",
    "For the loss function use the `BCEWithLogitsLoss` which takes as inputs the logits ($h^{(i)}$) and the targets ($y^{(i)}$) and then calculates the negative log loss by first converting the logits to probabilities by passing them through the sigmoid function.\n",
    "$$\n",
    "p^{(i)} = \\frac{1}{1 + e^{-h^{(i)}}} \\\\\n",
    "-\\mathcal L^{(i)} = - \\left[ y^{(i)} log(p^{(i)}) + (1 - y^{(i)}) log(1 - p^{(i)}) \\right] \\\\\n",
    "$$\n",
    "\n",
    "This means that my model does **not** have to output probabilities, so no need for the sigmoid activation on the final single unit. One weirdness about the way PyTorch has implemented this loss function is that it needs both the probabilities and targets as float values, even though the targets are clearly integers. For this reason I have to convert the targets to floats before calling this loss function.\n",
    "\n",
    "### In Summary\n",
    "  * Ensure that the model is **not** outputting a probability.\n",
    "  * Squeeze the output tensor along the 1st dimension before returning.\n",
    "  * Remember to cast the target values to float before calling the BCE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = t.nn.Sequential(\n",
    "            t.nn.Linear(7, 8),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7]) torch.Size([5])\n",
      "tensor([0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 5\n",
    "inputs = t.randn(batch_size, 7)\n",
    "targets = t.randint(0, 2, (batch_size,))\n",
    "print(inputs.shape, targets.shape)\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5676, 0.2446, 0.0893, 0.2848, 0.8677], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BinaryClassifier()\n",
    "outputs = model(inputs)\n",
    "print(outputs.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6491, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_loss = t.nn.BCEWithLogitsLoss()\n",
    "bce_loss(outputs, targets.to(t.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "Most datasets will output a single scalar either $0, 1, \\cdots, c-1$ where $c$ is the number of classes. The model should output a row vector of values indicating the likelihood of each class. \n",
    "\n",
    "For the loss function use `CrossEntropyLoss`. This is similar to the `BCEWithLogitsLoss` in that it accepts the logits ($\\mathbf h^{(i)}$) as its input. It will convert the row of logits into probabilities using the softmax function and then calculate the negative log likelihood for each instance.\n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{h_k}}{\\sum_{j=1}^c e^{h_j}} \\\\\n",
    "-\\mathcal L = \\sum_{k=1}^c y_k log(p_k)\n",
    "$$\n",
    "\n",
    "### In Summary\n",
    "  * Ensure that the model is **not** outputting probability distributions across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassifier(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = t.nn.Sequential(\n",
    "            t.nn.Linear(7, 8),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Linear(8, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7]) torch.Size([5])\n",
      "tensor([2, 2, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "inputs = t.randn(batch_size, 7)\n",
    "targets = t.randint(0, 3, (batch_size,))\n",
    "print(inputs.shape, targets.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1355, -0.1946,  0.1838],\n",
       "        [-0.3887, -0.2140,  0.0636],\n",
       "        [-0.1810, -0.3151,  0.1867],\n",
       "        [-0.2574, -0.1620,  0.2319],\n",
       "        [-0.1269, -0.0753,  0.3005]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MulticlassClassifier()\n",
    "outputs = model(inputs)\n",
    "print(outputs.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0383, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = t.nn.CrossEntropyLoss()\n",
    "ce_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Most datasets will have a single float scalar as the target. This means that after being batched by a dataloader, the target will be a single row vector `torch.Size(batch_size)`. This is just like the binary classification case. And I'll use the same method of ensuring that my model output is also a single row vector.\n",
    "\n",
    "For the loss function use the `MSELoss` which accepts two row vectors - both floats. Unlike the binary classification case, this is not a problem because the targets are already floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = t.nn.Sequential(\n",
    "            t.nn.Linear(7, 8),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7]) torch.Size([5])\n",
      "tensor([0.5479, 0.9801, 0.7519, 0.5885, 0.8585])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "inputs = t.randn(batch_size, 7)\n",
    "targets = t.rand((batch_size,))\n",
    "print(inputs.shape, targets.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0138, -0.3122, -0.2345, -0.1548, -0.1343],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Regressor()\n",
    "outputs = model(inputs)\n",
    "print(outputs.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8992, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = t.nn.MSELoss()\n",
    "mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:21:25) [Clang 14.0.4 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "248aaaef2ce54d580c63d0621ea4262f68d22105863a800468921850ea527b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
