{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "[Tutorial](https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html)\n",
    "\n",
    "Each filter needs to have the same number of channels as the input tensor. Each filter will create a single output channel. The number of filters determine the number of output channels. All the filters taken together are usually called \"filter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "To use F.conv2d we need the input to be of shape minibatch×in_channels×iH×iW and the filter to be of the shape out_channels×in_channels×kH×kW. In the simple case of simply convolving 2D tensor with a 2D filter, we use the minibatch size, in_channels, and out_channels all as 1. This is the reason to reshape the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[3., 0., 1., 2., 7., 4.],\n",
    "                  [1., 5., 8., 9., 3., 1.],\n",
    "                  [2., 7., 2., 5., 1., 3.],\n",
    "                  [0., 1., 3., 1., 7., 8.],\n",
    "                  [4., 2., 1., 6., 2., 8.],\n",
    "                  [2., 4., 5., 2., 3., 9.]])\n",
    "x = x.view(1, 1, 6, 6)\n",
    "\n",
    "w = torch.tensor([[2., 0., -2.],\n",
    "                  [2., 0., -2.],\n",
    "                  [2., 0., -2.]])\n",
    "w = w.view(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-10.,  -8.,   0.,  16.],\n",
       "          [-20.,  -4.,   4.,   6.],\n",
       "          [  0.,  -4.,  -8., -14.],\n",
       "          [ -6.,  -4.,  -6., -32.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0., 180., 180.,   0.],\n",
       "          [  0., 180., 180.,   0.],\n",
       "          [  0., 180., 180.,   0.],\n",
       "          [  0., 180., 180.,   0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[30., 30., 30., 0., 0., 0.],\n",
    "                  [30., 30., 30., 0., 0., 0.],\n",
    "                  [30., 30., 30., 0., 0., 0.],\n",
    "                  [30., 30., 30., 0., 0., 0.],\n",
    "                  [30., 30., 30., 0., 0., 0.],\n",
    "                  [30., 30., 30., 0., 0., 0.]])\n",
    "x = x.view(1, 1, 6, 6)\n",
    "\n",
    "F.conv2d(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*5*3*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_filter = np.array([\n",
    "    [1., 0., -1.],\n",
    "    [1., 0., -1.],\n",
    "    [1., 0., -1.]\n",
    "])\n",
    "red_filter_full = torch.tensor(red_filter.reshape(1, 1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.array([\n",
    "    [10., 10., 10., 0., 0., 0.],\n",
    "    [10., 10., 10., 0., 0., 0.],\n",
    "    [10., 10., 10., 0., 0., 0.],\n",
    "    [10., 10., 10., 0., 0., 0.],\n",
    "    [10., 10., 10., 0., 0., 0.],\n",
    "    [10., 10., 10., 0., 0., 0.]\n",
    "])\n",
    "red_batch_of_one = torch.tensor(red.reshape((1, 1, 6, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_out = F.conv2d(red_batch_of_one, red_filter_full)\n",
    "print(red_out.shape)\n",
    "red_out[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_filter = np.array([\n",
    "    [2., 0., -2.],\n",
    "    [2., 0., -2.],\n",
    "    [2., 0., -2.]\n",
    "])\n",
    "green_filter_full = torch.tensor(green_filter.reshape(1, 1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = np.array([\n",
    "    [30., 30., 30., 0., 0., 0.],\n",
    "    [30., 30., 30., 0., 0., 0.],\n",
    "    [30., 30., 30., 0., 0., 0.],\n",
    "    [30., 30., 30., 0., 0., 0.],\n",
    "    [30., 30., 30., 0., 0., 0.],\n",
    "    [30., 30., 30., 0., 0., 0.]\n",
    "])\n",
    "green_batch_of_one = torch.tensor(green.reshape((1, 1, 6, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0., 180., 180.,   0.],\n",
       "        [  0., 180., 180.,   0.],\n",
       "        [  0., 180., 180.,   0.],\n",
       "        [  0., 180., 180.,   0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_out = F.conv2d(green_batch_of_one, green_filter_full)\n",
    "print(green_out.shape)\n",
    "green_out[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filter = np.array([\n",
    "    [0.5, 0., -0.5],\n",
    "    [0.5, 0., -0.5],\n",
    "    [0.5, 0., -0.5]\n",
    "])\n",
    "blue_filter_full = torch.tensor(blue_filter.reshape(1, 1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = np.array([\n",
    "    [20., 20., 20., 0., 0., 0.],\n",
    "    [20., 20., 20., 0., 0., 0.],\n",
    "    [20., 20., 20., 0., 0., 0.],\n",
    "    [20., 20., 20., 0., 0., 0.],\n",
    "    [20., 20., 20., 0., 0., 0.],\n",
    "    [20., 20., 20., 0., 0., 0.]\n",
    "])\n",
    "blue_batch_of_one = torch.tensor(blue.reshape((1, 1, 6, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.],\n",
       "        [ 0., 30., 30.,  0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_out = F.conv2d(blue_batch_of_one, blue_filter_full)\n",
    "print(blue_out.shape)\n",
    "blue_out[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 6, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.tensor(np.expand_dims(np.stack([red, green, blue]), axis=0))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_filter = torch.tensor(np.expand_dims(np.stack([red_filter, green_filter, blue_filter]), axis=0))\n",
    "img_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0., 240., 240.,   0.],\n",
       "        [  0., 240., 240.,   0.],\n",
       "        [  0., 240., 240.,   0.],\n",
       "        [  0., 240., 240.,   0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_out = F.conv2d(img, img_filter)\n",
    "print(img_out.shape)\n",
    "img_out[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0., 240., 240.,   0.],\n",
       "          [  0., 240., 240.,   0.],\n",
       "          [  0., 240., 240.,   0.],\n",
       "          [  0., 240., 240.,   0.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_out + green_out + blue_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding \n",
    "With no padding the center pixels get a bigger say in the convolved output tensor. With padding, I can structure it so that all the pixels get more or less an equal say. This can be useful if there are features at the border need to be captured.\n",
    "\n",
    "![padding](./padding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the different padding types that I know of:\n",
    "  * **Valid padding**: This is when there is no padding and the output tensor is smaller than the input tensor.\n",
    "  * **Half or same padding**: This is when the input tensor is framed by zeros. As many rows/cols are added s.t the output tensor has the same dimensions as the input tensor. Most libraries calculate the exact number of \"frames\" to add.\n",
    "  * **Full padding**: This is when all the pixels are counted the same number of times. Again, most libraries calculate the exact number of \"frames\" to surround the input tensor with.\n",
    "\n",
    "Half and Full padding are instances of \"zero\" padding, because the pad value is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling is a sort of downsampling. It is used to boil the input tensor to its essentials. The pooled or downsampled tensor is then easier to run subsequent computations than the full tensor. Pooling does not have any learned parameters. It is still differentiable. E.g., the max pooling operation when back propagated will take the gradient as-is for all the pixels that were selected and 0 for all other pixels. See [this stackoverflow question](https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers).\n",
    "\n",
    "![pooled](./pooled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 2.],\n",
       "          [6., 3.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 3., 2., 1.],\n",
    "                  [2., 9., 1., 1.],\n",
    "                  [1., 3., 2., 3.],\n",
    "                  [5., 6., 1., 2.]])\n",
    "x = x.view(1, 1, 4, 4)\n",
    "\n",
    "F.max_pool2d(x, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 9., 5.],\n",
       "          [9., 9., 5.],\n",
       "          [8., 6., 9.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 3., 2., 1., 3.],\n",
    "                  [2., 9., 1., 1., 5.],\n",
    "                  [1., 3., 2., 3., 2.],\n",
    "                  [8., 3., 5., 1., 0.],\n",
    "                  [5., 6., 1., 2., 9.]])\n",
    "x = x.view(1, 1, 5, 5)\n",
    "F.max_pool2d(x, kernel_size=3, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "It is in a sense opposite of pooling, it can be used to \"blow up\" a tensor. Just like pooling there are no learnable parameters and this operation too is differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[20., 20., 45., 45.],\n",
       "          [20., 20., 45., 45.],\n",
       "          [10., 10., 43., 43.],\n",
       "          [10., 10., 43., 43.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[20, 45],\n",
    "                  [10, 43]], dtype=torch.float32)\n",
    "x = x.view(1, 1, 2, 2)\n",
    "F.interpolate(x, scale_factor=2, mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avilayparekh/miniconda3/envs/dev/lib/python3.10/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[20.0000, 26.2500, 38.7500, 45.0000],\n",
       "          [17.5000, 24.2500, 37.7500, 44.5000],\n",
       "          [12.5000, 20.2500, 35.7500, 43.5000],\n",
       "          [10.0000, 18.2500, 34.7500, 43.0000]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(x, scale_factor=2, mode=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Convolution\n",
    "\n",
    "Think of this as upsampling with learned parameters. These are also sometimes (incorrectly) called as deconvolutions.\n",
    "\n",
    "![transpose](./transpose.png)\n",
    "\n",
    "So far I have seen only square kernels and custom square strides. The output for the following input:\n",
    "  * Square input of size $i \\times i$\n",
    "  * Square kernel of size $k \\times k$\n",
    "  * Square stride of size $s \\times s$\n",
    "\n",
    "will be a square tensor -\n",
    "$$\n",
    "s(i - 1) + k\n",
    "$$\n",
    "\n",
    "The problem with transposed convolutions is that the center pixel is getting influenced by all the pixels. This can lead to the so-called [checkerboard pattern](https://distill.pub/2016/deconv-checkerboard/) in the output tensor. Doing a (parameterless) upsampling followed by same convolution is one way to combat this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2, 10,  8],\n",
       "          [ 1,  9,  8],\n",
       "          [ 0,  2,  2]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 4],\n",
    "    [0, 2]\n",
    "]).view(1, 1, 2, 2)\n",
    "w = torch.tensor([\n",
    "    [2, 2],\n",
    "    [1, 1]\n",
    "]).view(1, 1, 2, 2)\n",
    "\n",
    "F.conv_transpose2d(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2, 2, 8, 8],\n",
       "          [1, 1, 4, 4],\n",
       "          [0, 0, 4, 4],\n",
       "          [0, 0, 2, 2]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv_transpose2d(x, w, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "784114715d987f376fd768172d6fb64b1fb77afe12a7e2b607779dcdec22a151"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
