{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as td\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistributedSampler\n",
    "As with all samplers, it only makes sense to use this sampler with map-style datasets. It is supposed to be used in a data-parallel setting where there are multiple processes with a copy of the DNN processing a subset of the overall data. When the `DistributedSampler` is initialized in one of the processes which is identified by its rank, the sampler does the following:\n",
    "\n",
    "  1. Shuffles the indexes in the dataset (by default `shuffle=True` in the ctor).\n",
    "  2. Gets the `rank` and the `world_size` either from the environment variables or from the passed in arguments.\n",
    "  3. Chops up the shuffled list of indexes into `world_size` chunks and allocates each chunk to a `rank`.\n",
    "\n",
    "Now, when the data loader asks it to give an index so it can get that instance from the dataset, the sampler will dole out the indexes allocated to its own `rank`. As an aside, this sampler is not a batched sampler, so the data loader will still have to use some way of batching individual instances, e.g., using the default `BatchSampler`.\n",
    "\n",
    "A sampler is initialized in each process of every rank. The only way the same chunks are allocated to the same `rank` is if the indexes are shuffled using the same random seed. For this reason, the ctor takes a default seed of $0$, but of course I can give it any seed I want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(td.Dataset):\n",
    "    def __init__(self, m=100, n=3):\n",
    "        self._x = np.array([np.full(n, i + 1) for i in range(m)])\n",
    "        self._y = np.random.choice([0, 1], size=m, p=[0.7, 0.3])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._x[idx], self._y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1] 1\n",
      "[2 2 2] 0\n",
      "[3 3 3] 1\n",
      "[4 4 4] 1\n",
      "[5 5 5] 1\n",
      "[6 6 6] 0\n",
      "[7 7 7] 0\n",
      "[8 8 8] 0\n",
      "[9 9 9] 0\n",
      "[10 10 10] 0\n"
     ]
    }
   ],
   "source": [
    "ds = Simple(m=10)\n",
    "for x, y in ds:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the related concepts in this notebook I'll pass in the world size and the rank of each sampler in its ctor. Typically these don't need to be specified because they are picked up from the environment variables. In the call to the `DistributedSampler` ctor in the cells below, `shuffle=True` and `seed=0` is passed in by default. Because of this seed, everytime I run these two cells, rank 0 and rank 1 will have the same \"shuffled\" data.\n",
    "\n",
    "##### Rank 0 data:\n",
    "\n",
    "```\n",
    "[tensor([[20, 20, 20],\n",
    "        [ 7,  7,  7],\n",
    "        [ 6,  6,  6],\n",
    "        [12, 12, 12]]), tensor([1, 0, 0, 0])]\n",
    "[tensor([[15, 15, 15],\n",
    "        [22, 22, 22],\n",
    "        [16, 16, 16],\n",
    "        [23, 23, 23]]), tensor([0, 1, 0, 1])]\n",
    "[tensor([[14, 14, 14],\n",
    "        [ 8,  8,  8],\n",
    "        [ 4,  4,  4],\n",
    "        [21, 21, 21]]), tensor([1, 1, 0, 0])]\n",
    "[tensor([[1, 1, 1]]), tensor([0])]\n",
    "```\n",
    "\n",
    "##### Rank 1 data:\n",
    "\n",
    "```\n",
    "[tensor([[17, 17, 17],\n",
    "        [18, 18, 18],\n",
    "        [25, 25, 25],\n",
    "        [ 9,  9,  9]]), tensor([1, 0, 0, 1])]\n",
    "[tensor([[13, 13, 13],\n",
    "        [ 3,  3,  3],\n",
    "        [ 2,  2,  2],\n",
    "        [11, 11, 11]]), tensor([0, 0, 1, 0])]\n",
    "[tensor([[10, 10, 10],\n",
    "        [19, 19, 19],\n",
    "        [ 5,  5,  5],\n",
    "        [24, 24, 24]]), tensor([1, 0, 1, 0])]\n",
    "[tensor([[20, 20, 20]]), tensor([0])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[20, 20, 20],\n",
      "        [ 7,  7,  7],\n",
      "        [ 6,  6,  6],\n",
      "        [12, 12, 12]]), tensor([1, 0, 0, 0])]\n",
      "[tensor([[15, 15, 15],\n",
      "        [22, 22, 22],\n",
      "        [16, 16, 16],\n",
      "        [23, 23, 23]]), tensor([0, 1, 0, 1])]\n",
      "[tensor([[14, 14, 14],\n",
      "        [ 8,  8,  8],\n",
      "        [ 4,  4,  4],\n",
      "        [21, 21, 21]]), tensor([1, 1, 0, 0])]\n",
      "[tensor([[1, 1, 1]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "ds0 = Simple(m=25)\n",
    "sampler0 = td.distributed.DistributedSampler(ds0, num_replicas=2, rank=0)\n",
    "dl0 = td.DataLoader(ds0, batch_size=4, sampler=sampler0)\n",
    "for batch in dl0:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[17, 17, 17],\n",
      "        [18, 18, 18],\n",
      "        [25, 25, 25],\n",
      "        [ 9,  9,  9]]), tensor([1, 0, 0, 1])]\n",
      "[tensor([[13, 13, 13],\n",
      "        [ 3,  3,  3],\n",
      "        [ 2,  2,  2],\n",
      "        [11, 11, 11]]), tensor([0, 0, 1, 0])]\n",
      "[tensor([[10, 10, 10],\n",
      "        [19, 19, 19],\n",
      "        [ 5,  5,  5],\n",
      "        [24, 24, 24]]), tensor([1, 0, 1, 0])]\n",
      "[tensor([[20, 20, 20]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "ds1 = Simple(m=25)\n",
    "sampler1 = td.distributed.DistributedSampler(ds1, num_replicas=2, rank=1)\n",
    "dl1 = td.DataLoader(ds1, batch_size=4, sampler=sampler1)\n",
    "for batch in dl1:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the indexes are partitioned across the different processes, try this out without shuffling. I'll see that the sampler assignes the indexes in a round robin fashion to each rank. Of course if `shuffle=False` the seed does not matter anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1,  1,  1],\n",
      "        [ 4,  4,  4],\n",
      "        [ 7,  7,  7],\n",
      "        [10, 10, 10]]), tensor([0, 0, 0, 0])]\n",
      "[tensor([[13, 13, 13],\n",
      "        [16, 16, 16],\n",
      "        [19, 19, 19],\n",
      "        [22, 22, 22]]), tensor([1, 0, 1, 1])]\n",
      "[tensor([[25, 25, 25]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "ds0 = Simple(m=25)\n",
    "sampler0 = td.distributed.DistributedSampler(ds0, num_replicas=3, rank=0, shuffle=False)\n",
    "dl0 = td.DataLoader(ds0, batch_size=4, sampler=sampler0)\n",
    "for batch in dl0:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 2,  2,  2],\n",
      "        [ 5,  5,  5],\n",
      "        [ 8,  8,  8],\n",
      "        [11, 11, 11]]), tensor([0, 1, 0, 0])]\n",
      "[tensor([[14, 14, 14],\n",
      "        [17, 17, 17],\n",
      "        [20, 20, 20],\n",
      "        [23, 23, 23]]), tensor([0, 0, 1, 1])]\n",
      "[tensor([[1, 1, 1]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "ds1 = Simple(m=25)\n",
    "sampler1 = td.distributed.DistributedSampler(ds1, num_replicas=3, rank=1, shuffle=False)\n",
    "dl1 = td.DataLoader(ds1, batch_size=4, sampler=sampler1)\n",
    "for batch in dl1:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 3,  3,  3],\n",
      "        [ 6,  6,  6],\n",
      "        [ 9,  9,  9],\n",
      "        [12, 12, 12]]), tensor([0, 0, 1, 1])]\n",
      "[tensor([[15, 15, 15],\n",
      "        [18, 18, 18],\n",
      "        [21, 21, 21],\n",
      "        [24, 24, 24]]), tensor([0, 0, 0, 1])]\n",
      "[tensor([[2, 2, 2]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "ds2 = Simple(m=25)\n",
    "sampler2 = td.distributed.DistributedSampler(ds2, num_replicas=3, rank=2, shuffle=False)\n",
    "dl2 = td.DataLoader(ds2, batch_size=4, sampler=sampler2)\n",
    "for batch in dl2:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "248aaaef2ce54d580c63d0621ea4262f68d22105863a800468921850ea527b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
