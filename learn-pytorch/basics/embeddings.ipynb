{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchtext as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = path.expanduser(\"~/mldata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "An embedding is a look up table, that when given a word/token index, will return the vector representation of that word/token. The `num_embeddings` param tells the `Embedding` class big is this lookup table. This is set to the size of our vocabulary. The `embedding_dim` param specifies the size of the resulting word vector. By default, this embedding will have random values for each word/token index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100_000\n",
    "embedding = t.nn.Embedding(num_embeddings=vocab_size, embedding_dim=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch of 3 sequences, each sequence being 2 words long. Such a tensor has BATCH_SIZE x SEQ_LEN dimensions.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "hello & world \\\\\n",
    "goodbye & everybody \\\\\n",
    "cookie & monster \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When I pass this through the embedding layer, it should replace each word/token with its corresponding vector. The output tensor will have dimensions BATCH_SIZE x SEQ_LEN x EMBEDDING_DIM. This is important, because for RNN layers, the default dimensions are SEQ_LEN x BATCH_SIZE x INPUT_DIM\n",
    "\n",
    "For now we don't have a real dataset, so lets pretend that these words have the following index values:\n",
    "  * hello => 12545\n",
    "  * world => 51\n",
    "  * goodbye => 7373\n",
    "  * everybody => 7771\n",
    "  * cookie => 17185\n",
    "  * monster => 6290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = t.tensor([\n",
    "    [12545,    51],\n",
    "    [ 7373,  7771],\n",
    "    [17185,  6290]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 11])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embedding(contents)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the generated embedding, the word vector corrosponding to `hello` is `emb[0][0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6146, -0.9629,  0.4635, -2.5220,  1.6283, -0.5316,  0.3511,  0.7853,\n",
       "        -0.8683,  1.1452, -1.1283], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Embeddings\n",
    "One way is to train the embeddings along with the rest of my network. For big datasets with a very specific vocabulary, e.g., legal documents, this works well. However for most other problems it makes sense to use existing pre-trained word vectors like those from GloVe.\n",
    "\n",
    "For this notebook, lets use the `glove.6B.100d` embeddings, which has around 6B unique tokens and the embeddings have a size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avilay/mldata/glove/glove.6B.zip: 862MB [02:39, 5.41MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:05<00:00, 76183.61it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_datapath = path.join(DATAROOT, \"glove\")\n",
    "glove = tt.vocab.GloVe(name=\"6B\", dim=100, cache=glove_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
       "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
       "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
       "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
       "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
       "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
       "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
       "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
       "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
       "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
       "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
       "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
       "        -0.5203, -0.1459,  0.8278,  0.2706])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The glove dataset is very simple. Each line starts with the word/token followed by the vector values in the same line. So to get the word vector for `the`, which happens to be the first word in the dataset, just read the first line of `glove.6B.100d.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_glove_the = t.tensor([-0.038194,-0.24487,0.72812,-0.39961,0.083172,0.043953,-0.39141,0.3344,-0.57545,0.087459,0.28787,-0.06731,0.30906,-0.26384,-0.13231,-0.20757,0.33395,-0.33848,-0.31743,-0.48336,0.1464,-0.37304,0.34577,0.052041,0.44946,-0.46971,0.02628,-0.54155,-0.15518,-0.14107,-0.039722,0.28277,0.14393,0.23464,-0.31021,0.086173,0.20397,0.52624,0.17164,-0.082378,-0.71787,-0.41531,0.20335,-0.12763,0.41367,0.55187,0.57908,-0.33477,-0.36559,-0.54857,-0.062892,0.26584,0.30205,0.99775,-0.80481,-3.0243,0.01254,-0.36942,2.2167,0.72201,-0.24978,0.92136,0.034514,0.46745,1.1079,-0.19358,-0.074575,0.23353,-0.052062,-0.22044,0.057162,-0.15806,-0.30798,-0.41625,0.37972,0.15006,-0.53212,-0.2055,-1.2526,0.071624,0.70565,0.49744,-0.42063,0.26148,-1.538,-0.30223,-0.073438,-0.28312,0.37104,-0.25217,0.016215,-0.017099,-0.38984,0.87424,-0.72569,-0.51058,-0.52028,-0.1459,0.8278,0.27062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[glove.stoi[\"the\"]].allclose(expected_glove_the)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can create the `Embedding` layer with these pre-trained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = t.nn.Embedding.from_pretrained(glove.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13075,    85],\n",
       "        [10926,  2587],\n",
       "        [13816,  7519]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = t.tensor([\n",
    "    [glove.stoi[\"hello\"], glove.stoi[\"world\"]],\n",
    "    [glove.stoi[\"goodbye\"], glove.stoi[\"everybody\"]],\n",
    "    [glove.stoi[\"cookie\"], glove.stoi[\"monster\"]],\n",
    "])\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when I pass this tensor through the embedding layer, it will replace 13705 with the word vector for `hello`, 10926 with the word vector for `goodbye`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello = glove.vectors[glove.stoi[\"hello\"]]\n",
    "world = glove.vectors[glove.stoi[\"world\"]]\n",
    "goodbye = glove.vectors[glove.stoi[\"goodbye\"]]\n",
    "everybody = glove.vectors[glove.stoi[\"everybody\"]]\n",
    "cookie = glove.vectors[glove.stoi[\"cookie\"]]\n",
    "monster = glove.vectors[glove.stoi[\"monster\"]]\n",
    "\n",
    "exp = t.stack((\n",
    "    t.stack((hello, world)), \n",
    "    t.stack((goodbye, everybody)), \n",
    "    t.stack((cookie, monster))\n",
    "))\n",
    "exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embedding(contents)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.allclose(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5088,  0.6256,  0.4392])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0,1,-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "First assign all unique tokens in the corpus a unique index. The pseudocode for this will look something like -\n",
    "```python\n",
    "tok_to_idx = {}\n",
    "last_idx = 0\n",
    "for token in corpus:\n",
    "    if token not in tok_to_idx:\n",
    "        tok_to_idx[token] = last_idx\n",
    "        last_idx += 1\n",
    "```\n",
    "\n",
    "The actual input that we want to feed to our model is -\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "hello & world \\\\\n",
    "goodbye & everybody \\\\\n",
    "cookie & monster \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "I'll need to convert the input tokens to their corresponding indexes. The pseudocode will look something like -\n",
    "```python\n",
    "def convert(input : List[List[str]], tok_to_idx: Dict[str, int]):\n",
    "    batch_size = len(input)\n",
    "    n_features = len(input[0])\n",
    "    return t.tensor([[tok_to_idx[input[i][j]] for j in n_featurs] for i in batch_size])\n",
    "```\n",
    "\n",
    "For ranking and recommendation problems, the tokens can be **post ids** or **location ids** or **product ids**, etc. They may look numerical, but I need to map them to some index space. This is why it is best to treat these ids as tokens. After all of this I'll end up with with a $m \\times n$ integer tensor where $m$ is the batch size and $n$ is the number of categorical features.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "13075 & 85 \\\\\n",
    "10926 & 2587 \\\\\n",
    "13816 & 7519\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When I pass this input to the `Embedding` layer, it will convert each idx into the corresponding embedding vector and return a $m \\times n \\times d$ where $d$ is the embedding dimension.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.2669 & 0.3963 & 0.6169 & \\cdots & 0.3584 & -0.4846 & 0.3073 \\\\\n",
    "0.4918 & 1.1164 & 1.1424 & \\cdots & -0.5088 & 0.6256 & 0.4392 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\begin{bmatrix}\n",
    "0.2669 & 0.3963 & 0.6169 & \\cdots & 0.3584 & -0.4846 & 0.3073 \\\\\n",
    "0.4918 & 1.1164 & 1.1424 & \\cdots & -0.5088 & 0.6256 & 0.4392 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\begin{bmatrix}\n",
    "0.2669 & 0.3963 & 0.6169 & \\cdots & 0.3584 & -0.4846 & 0.3073 \\\\\n",
    "0.4918 & 1.1164 & 1.1424 & \\cdots & -0.5088 & 0.6256 & 0.4392 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Bag\n",
    "\n",
    "A lot of times the value of a feature is not a single token, but multiple tokens. E.g., if my input consists of bi-grams then each position will have two tokens. If my feature is the posts that a user has liked in the last 5 days, it can be multiple post IDs. In this case I generally want to get the embeddings of each token from the embedding table and then reduce them somehow by summing, averaging, etc. Lets say my input is -\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 1 & 2 \\end{bmatrix} & \\begin{bmatrix}  \\end{bmatrix} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = t.nn.EmbeddingBag(num_embeddings=10, embedding_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3959, -0.2102,  0.1154],\n",
       "        [ 0.3190,  0.7162, -0.7253]], grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = t.tensor([1, 2, 4, 5, 4, 3, 2, 9], dtype=t.long)\n",
    "offsets = t.tensor([0, 4], dtype=t.long)\n",
    "embs = embeddings(input, offsets)\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using embedding with a real dataset\n",
    "So far I have been creating my `contents` matrix by hand and using the index values provided by the GloVe dataset. In reality, I'll have a text corpus and the vocabulary for that will be auto-genrated by PyTorch. In such cases, the words will have different indexes. E.g., in the AG News dataset, the word `the` has index 3, whereas in the GloVe dataset it has an index of 0. For such pre-existing vocabulary objects, I can load the vectors of a pre-trained word vector dataset and the `Vocab` object will automatically map the words to their vectors. The index will still be what was in the original vocab. E.g., in the AG News dataset, after loading the GloVe vectors, the index of `the` will still be 3, but now its vector value will be the GloVe word vector.\n",
    "\n",
    "When creating the `Embedding` object, I must take care to use the pre-trained vector from the vocab, and not from the glove vector.\n",
    "\n",
    "### Aug 20, 2022: Code in this section is broken because `torchtext` has taken a dependency on `torchdata` which seems to be broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IterableWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/avilay/projects/bitbucket/learn/learn-pytorch/basics/embeddings.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/avilay/projects/bitbucket/learn/learn-pytorch/basics/embeddings.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datapath \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mjoin(DATAROOT, \u001b[39m\"\u001b[39m\u001b[39mCoLA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/avilay/projects/bitbucket/learn/learn-pytorch/basics/embeddings.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainset, testset \u001b[39m=\u001b[39m tt\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCoLA(datapath)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/avilay/projects/bitbucket/learn/learn-pytorch/basics/embeddings.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(trainset), \u001b[39mlen\u001b[39m(testset))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dev/lib/python3.10/site-packages/torchtext/data/datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(new_root):\n\u001b[1;32m    192\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(new_root, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m fn(root\u001b[39m=\u001b[39;49mnew_root, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dev/lib/python3.10/site-packages/torchtext/data/datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m _check_default_set(split, splits, fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result\u001b[39m.\u001b[39mappend(fn(root, item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m _wrap_datasets(\u001b[39mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dev/lib/python3.10/site-packages/torchtext/datasets/cola.py:82\u001b[0m, in \u001b[0;36mCoLA\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchdata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m url_dp \u001b[39m=\u001b[39m IterableWrapper([URL])\n\u001b[1;32m     83\u001b[0m cache_compressed_dp \u001b[39m=\u001b[39m url_dp\u001b[39m.\u001b[39mon_disk_cache(\n\u001b[1;32m     84\u001b[0m     filepath_fn\u001b[39m=\u001b[39mpartial(_filepath_fn, root),\n\u001b[1;32m     85\u001b[0m     hash_dict\u001b[39m=\u001b[39m{_filepath_fn(root): MD5},\n\u001b[1;32m     86\u001b[0m     hash_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m cache_compressed_dp \u001b[39m=\u001b[39m HttpReader(cache_compressed_dp)\u001b[39m.\u001b[39mend_caching(mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m, same_filepath_fn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IterableWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "datapath = path.join(DATAROOT, \"CoLA\")\n",
    "trainset, testset = tt.datasets.CoLA(datapath)\n",
    "print(len(trainset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_RawTextIterableDataset' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9e5261cda082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"the\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_RawTextIterableDataset' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "vocab = trainset.get_vocab()\n",
    "vocab.stoi[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially `vocab` does not have any vectors. All it has is the token and its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fa0564b4cf6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab.vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load the `glove` vectors, the vocab will have automatically map the right word indexes to the right vectors. As can be seen in the example below, the word with index 3 `the` is mapped to the right word vector from `glove` where it had the index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.load_vectors(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[3].allclose(expected_glove_the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = t.nn.Embedding.from_pretrained(vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = t.tensor([\n",
    "    [vocab.stoi[\"hello\"], vocab.stoi[\"world\"]],\n",
    "    [vocab.stoi[\"goodbye\"], vocab.stoi[\"everybody\"]],\n",
    "    [vocab.stoi[\"cookie\"], vocab.stoi[\"monster\"]]\n",
    "])\n",
    "print(contents.shape)\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embedding(contents)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the `contents` values, i.e., the word indexes are different, the embedding matrix is same as before, i.e., the word `hello` is still replaced with the word vector for `hello` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.allclose(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[253231109,         1],\n",
       "        [253311776,         1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.empty((2, 2), dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "248aaaef2ce54d580c63d0621ea4262f68d22105863a800468921850ea527b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
