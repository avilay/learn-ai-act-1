{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import torch as t\n",
    "# import torchtext as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = path.expanduser(\"~/mldata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "An embedding is a lookup table, that when given a token index, will return the vector representation of that token. \n",
    "\n",
    "![embedding_1](./imgs/embedding_1.png)\n",
    "\n",
    "It takes two parameters, `num_embeddings` is the height or the rows in the embedding table, and `embedding_dim` is the width or the cols.\n",
    "\n",
    "When initialized, the `Embedding` layer will have random embedding values, just like any other layer in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([5])\n",
      "tensor([-0.4781, -0.1330,  0.9880,  3.0628,  0.0660],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "3435\n",
      "torch.Size([5])\n",
      "tensor([-2.7041,  1.1003, -0.1545, -0.1265, -0.6299],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "988\n",
      "torch.Size([5])\n",
      "tensor([ 0.8695,  0.6965, -0.5266, -0.3331, -0.2609],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = t.nn.Embedding(num_embeddings=5_000, embedding_dim=5)\n",
    "for idx in [0, 3435, 988]:\n",
    "    emb = embedding(t.tensor(idx))\n",
    "    print(idx)\n",
    "    print(emb.shape)\n",
    "    print(emb)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all other layers, `Embedding` layer can accept a batch of indexes. Moreover, each row can have multiple indexes. If the input is an $m \\times n$ integer tensor, where $m$ is the batch size and $n$ is the number of indexes in each row, then the output will be $m \\times n \\times d$ where $d$ is the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.8387e-01, -7.7427e-01,  1.3158e+00,  1.4077e+00, -7.3324e-01],\n",
       "         [-7.7331e-01, -1.7010e-01,  8.7688e-01, -1.3446e+00,  6.4063e-01],\n",
       "         [-3.0518e-01,  8.4026e-01, -5.6898e-01,  9.8898e-01, -6.1587e-04]],\n",
       "\n",
       "        [[-5.8915e-03, -4.6279e-02, -2.0383e-01,  4.3963e-01,  1.2567e+00],\n",
       "         [ 1.9461e+00,  1.1002e+00, -5.0909e-01, -1.2554e+00,  1.7956e+00],\n",
       "         [ 9.0478e-01, -5.3915e-02, -4.1651e-01,  2.5856e-02, -3.2406e-01]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = t.tensor([\n",
    "    [0, 3435, 988],\n",
    "    [3840, 8, 2123]\n",
    "])\n",
    "emb = embedding(input)\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "Embeddings are used to model sparse or categorical features, including words in a text/language model. Lets understand each use case in more detail. Lets say I am an e-commerce website and my training set has the following features:\n",
    "  * Monthly average of transaction amount: float\n",
    "  * Monthly average of number of transactions: float\n",
    "  * Number of days since last transaction: int\n",
    "  * Product IDs of the last three purchased products: List[str]\n",
    "\n",
    "As can be seen from above, the first three features are my normal \"dense\" features, but the last feature is a **\"sparse\"** feature.\n",
    "\n",
    "Lets add another feature to our training set above which is the home state of the user. This is similar to a sparse feature, except instead of a list of strings, its dtype is a single string. This is often referred to as a **\"categorical\"** feature.\n",
    "\n",
    "Lets add in a final feature which is their review comments, more specifically, the first 5 words used by them. If they have less than 5 words, then we pad the end with empty tokens. This is a text or language feature.\n",
    "\n",
    "| avg_amt | avg_n | n_days | purchased          | state | review                                      |\n",
    "| ------- | ----- | ------ | ------------------ | ----- | ------------------------------------------- |\n",
    "| 10.23   | 3     | 2      | [q2x8, 9juk, 90u7] | WA    | [I, was, very, happy, with]       |\n",
    "| 28.21   | 10    | 0.5    | [89i2, a8p0, q2x8]  | CA    | [Very, good, quality, but, bad] |\n",
    "\n",
    "\n",
    "Language models are another setting where we see text features. Lets say my text is compsed of \"it was the best of times it was the worst of times\". I want my sequence length to be 5 and batch size to be 2, then my input will be -\n",
    "```\n",
    "[\"it\", \"was\", \"the\", \"best\", \"of\"]\n",
    "[\"of\", \"times\", \"it\", \"was\", \"the\"]\n",
    "```\n",
    "Just for interest my output in this case will be -\n",
    "```\n",
    "[\"was\", \"the\", \"best\", \"of\", \"times\"]\n",
    "[\"times\", \"it\", \"was\", \"the\", \"worst\"]\n",
    "```\n",
    "This is not germane to the discussion on embeddings.\n",
    "\n",
    "The string tokens in the above table will need to be converted to their corresponding indexes before being processed by the embedding layers. This can happen at a pre-processing step or for each batch. Regardless, we need a way to map the string token to its corresponding index. In the simple case each unique string token is given a unique index starting from 0. The pseudocode for creating this token to index mapping will look like -\n",
    "```python\n",
    "alltoks = set()\n",
    "for row in trainset:\n",
    "  toks = row[\"feature_name\"].strip().lower().split(\" \")\n",
    "  for tok in toks:\n",
    "    alltoks.add(tok)\n",
    "\n",
    "tok_to_idx = {}\n",
    "curr_idx = 0\n",
    "for tok in alltoks:\n",
    "  if tok not in tok_to_idx:\n",
    "    tok_to_idx[tok] = curr_idx\n",
    "    curr_idx += 1\n",
    "```\n",
    "However, for most sparse features the number of unique tokens are very large which will make the embedding table very big. E.g., for a very big e-commerce company, their product catalog could be in the millions. Another problem is that new items are always being added to the catalog, this will force to retrain our embeddings everytime a new product is added to the catalog. To workaround this problem we use the so-called hashing trick, where the token is hashed to a value within a reasonable range.\n",
    "\n",
    "Regardless of how this is done, we will eventually end up replacing the string tokens with their corresponding indexes. After replacement, the above table might look like -\n",
    "| avg_amt | avg_n | n_days | purchased          | state | review                                      |\n",
    "| ------- | ----- | ------ | ------------------ | ----- | ------------------------------------------- |\n",
    "| 10.23   | 3     | 2      | [39091, 72705, 31948] | 46    | [41, 15, 191, 1751, 17]       |\n",
    "| 28.21   | 10    | 0.5    | [65162, 31528, 39091]  | 4    | [191, 219, 1506, 34, 978] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model\n",
    "Typically we will have an embedding table per feature. In the above e-commerce example, we will have three embedding tables. The height of the product embedding table will be the number of products in our catalog, the height of the home state embedding table will be the total number of states in the country, and the height of the review comment table will be the vocabulary size of my entire training set. \n",
    "\n",
    "In the langauge model we will have a single embedding table whose size will be the same as the vocabulary size of my text corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Batch\n",
    "In \"regular\" DNN models my input batch is an $m \\times n$ float tensor where $m$ is the batch size and $n$ is the number of features. This entire tensor is fed to the first layer of my DNN. This setup won't work when I have mixed dense and sparse features. In the e-commerce model, I'll need to feed each sparse, categorical, and text feature to its own embedding table separately. \n",
    "\n",
    "To accomodate this, one way to structure my input batch is to have four tensors, the dense part which will be a float tensor $m \\times 3$ for the 3 dense features, the purchased tensor will be an integer tensor of $m \\times 3$ for the 3 product ids in each row, the state tensor will be an integer tensor of $m \\times 1$ and review tensor will be an integer tensor of $m \\times 5$ for the 5 words. The pseudocode for feeding an input batch into my DNN will be -\n",
    "```python\n",
    "for batch in traindl:\n",
    "    dense, purchased, state, review = batch\n",
    "    dense_out = model.mlp(dense)\n",
    "    purchased_embs_batch = model.purchased_embeddings(purchased)\n",
    "    state_embs_batch = model.state_embeddings(state)\n",
    "    review_embs_batch = model.review_embeddings(review)\n",
    "    # mix these outputs in some interaction arch\n",
    "```\n",
    "\n",
    "$$\n",
    "dense = \\begin{bmatrix}\n",
    "10.23 & 3.0 & 2.0 \\\\\n",
    "28.21 & 10.0 & 0.5 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "purchased = \\begin{bmatrix}\n",
    "39091 & 72705 & 31948 \\\\\n",
    "65162 & 31528 & 39091 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "state = \\begin{bmatrix}\n",
    "46 \\\\\n",
    "4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "review = \\begin{bmatrix}\n",
    "41 & 15 & 191 & 1751 & 17 \\\\\n",
    "191 & 219 & 1506 & 34 & 978 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note, this is just one way to structure my input batch, there are other ways as well.\n",
    "\n",
    "Of course for the language model example the \"regular\" setup continues to work because -\n",
    "  * The input is a $m \\times n$ integer tensor which is similar to the regular float input tensor.\n",
    "  * The first layer is typically only the embedding layer that can be fed this entire input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Like any other layer, `Embedding` layers can also be trained as part of our DNN, i.e., they have a `.backward()` method, optimizers know how to update the weights, etc. However, in a lot of cases the embeddings are typically trained in some upstream model and are only used in the main model during forward propagation. The `Embedding` class has a conveinece factory method to load pre-trained embeddings.\n",
    "\n",
    "Lets say I have a 100,000 items in my product catalog and my embedding dim is 8. Further lets say I have obtained pre-trained product embeddings from some upstream model somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_product_embs = t.rand((100_000, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100000, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchased_embeddings = t.nn.Embedding.from_pretrained(pre_trained_product_embs)\n",
    "purchased_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchased_batch = t.tensor([\n",
    "    [39091, 72705, 31948],\n",
    "    [65162, 31528, 39091]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3683, 0.0027, 0.2053, 0.2611, 0.9904, 0.2372, 0.4524, 0.8966],\n",
       "         [0.6351, 0.1236, 0.1456, 0.4281, 0.3921, 0.0811, 0.7195, 0.9048],\n",
       "         [0.3984, 0.5188, 0.7603, 0.0374, 0.8143, 0.3342, 0.2345, 0.0059]],\n",
       "\n",
       "        [[0.3247, 0.1131, 0.2974, 0.2839, 0.4851, 0.7985, 0.4210, 0.8215],\n",
       "         [0.4340, 0.3442, 0.1957, 0.5510, 0.9181, 0.1466, 0.6244, 0.2238],\n",
       "         [0.3683, 0.0027, 0.2053, 0.2611, 0.9904, 0.2372, 0.4524, 0.8966]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchased_embs_batch = purchased_embeddings(purchased_batch)\n",
    "purchased_embs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3683, 0.0027, 0.2053, 0.2611, 0.9904, 0.2372, 0.4524, 0.8966],\n",
       "         [0.6351, 0.1236, 0.1456, 0.4281, 0.3921, 0.0811, 0.7195, 0.9048],\n",
       "         [0.3984, 0.5188, 0.7603, 0.0374, 0.8143, 0.3342, 0.2345, 0.0059]],\n",
       "\n",
       "        [[0.3247, 0.1131, 0.2974, 0.2839, 0.4851, 0.7985, 0.4210, 0.8215],\n",
       "         [0.4340, 0.3442, 0.1957, 0.5510, 0.9181, 0.1466, 0.6244, 0.2238],\n",
       "         [0.3683, 0.0027, 0.2053, 0.2611, 0.9904, 0.2372, 0.4524, 0.8966]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_39091 = pre_trained_product_embs[39091]\n",
    "emb_72705 = pre_trained_product_embs[72705]\n",
    "emb_31948 = pre_trained_product_embs[31948]\n",
    "emb_65162 = pre_trained_product_embs[65162]\n",
    "emb_31528 = pre_trained_product_embs[31528]\n",
    "\n",
    "expected_purchased_embs_batch = t.stack((\n",
    "    t.stack((emb_39091, emb_72705, emb_31948)),\n",
    "    t.stack((emb_65162, emb_31528, emb_39091))\n",
    "))\n",
    "expected_purchased_embs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_purchased_embs_batch.allclose(purchased_embs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was a roundabout and usage specific way of seeing that the pretrained embeddings are the captured correctly in the embedding layer, i.e., `pre_trained_product_embs[i] == purchased_embeddings(t.tensor(i))`. Below is a more direct verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.allclose(pre_trained_product_embs[39091], purchased_embeddings(t.tensor(39091)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings\n",
    "A common set of pre-trained embeddings for English is GloVe. For this notebook lets use the `glove.6B.100d` embeddings which has around six billion unique tokens and the embedding dimension is 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_datapath = path.join(DATAROOT, \"glove\")\n",
    "glove = tt.vocab.GloVe(name=\"6B\", dim=100, cache=glove_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe dataset is very simple. Each line starts with the word or token followed by the vector values in the same line. Here is what the first line looks like -\n",
    "\n",
    "```shell\n",
    "(base) ‡•ê  glove $ head -1 glove.6B.100d.txt\n",
    "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
    "```\n",
    "\n",
    "The `GloVe` class has a convenience dict called `stoi` to get the index of any given string in its corpus. The pre-trained embeddings are in a tensor called `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
       "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
       "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
       "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
       "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
       "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
       "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
       "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
       "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
       "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
       "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
       "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
       "        -0.5203, -0.1459,  0.8278,  0.2706])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(400000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_embeddings = t.nn.Embedding.from_pretrained(glove.vectors)\n",
    "review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  41,   15,  191, 1751,   17],\n",
       "        [ 191,  219, 1506,   34,  978]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_batch = t.tensor([\n",
    "    [glove.stoi[\"i\"], glove.stoi[\"was\"], glove.stoi[\"very\"], glove.stoi[\"happy\"], glove.stoi[\"with\"]],\n",
    "    [glove.stoi[\"very\"], glove.stoi[\"good\"], glove.stoi[\"quality\"], glove.stoi[\"but\"], glove.stoi[\"bad\"]],\n",
    "])\n",
    "review_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_embs_batch = review_embeddings(review_batch)\n",
    "review_embs_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_i = glove.vectors[glove.stoi[\"i\"]]\n",
    "emb_was = glove.vectors[glove.stoi[\"was\"]]\n",
    "emb_very = glove.vectors[glove.stoi[\"very\"]]\n",
    "emb_happy = glove.vectors[glove.stoi[\"happy\"]]\n",
    "emb_with = glove.vectors[glove.stoi[\"with\"]]\n",
    "emb_good = glove.vectors[glove.stoi[\"good\"]]\n",
    "emb_quality = glove.vectors[glove.stoi[\"quality\"]]\n",
    "emb_but = glove.vectors[glove.stoi[\"but\"]]\n",
    "emb_bad = glove.vectors[glove.stoi[\"bad\"]]\n",
    "\n",
    "expected_review_embs_batch = t.stack((\n",
    "    t.stack((emb_i, emb_was, emb_very, emb_happy, emb_with)),\n",
    "    t.stack((emb_very, emb_good, emb_quality, emb_but, emb_bad))\n",
    "))\n",
    "expected_review_embs_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_review_embs_batch.allclose(review_embs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bags\n",
    "As can be seen in the examples above, each row of sparse features in the batch results in multiple vectors after the embedding lookup. E.g., the first row of the purchased feature was $\\begin{bmatrix}39091 & 72705 & 31948 \\end{bmatrix}$ but it becomes -\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.6127 & 0.6788 & 0.9122 & 0.5206 & 0.8504 & 0.7362 & 0.6871 & 0.4667 \\\\\n",
    "0.1555 & 0.8719 & 0.0692 & 0.1788 & 0.3327 & 0.0774 & 0.6614 & 0.0698 \\\\\n",
    "0.4526 & 0.5650 & 0.5481 & 0.0946 & 0.5106 & 0.6710 & 0.1706 & 0.3724 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "after embedding lookup. A very common next step is to reduce these multiple vectors back into a single vector. This can be done by adding the vectors up, or by averaging them, etc. The pesudocode for this then will be -\n",
    "```python\n",
    "purchased_embs = model.purchased_embeddings(purchased)\n",
    "purchased_vec = t.sum(purchased_embs, axis=1)\n",
    "```\n",
    "Because this is such a common step, PyTorch has as convenience class called `EmbeddingBag` to do this. It is also more efficient than the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39091, 72705, 31948],\n",
       "        [65162, 31528, 39091]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchased_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4017, 0.6451, 1.1112, 0.7266, 2.1968, 0.6525, 1.4064, 1.8073],\n",
       "        [1.1270, 0.4600, 0.6984, 1.0960, 2.3936, 1.1823, 1.4978, 1.9419]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_purchased_vecs = t.sum(purchased_embeddings(purchased_batch), axis=1)\n",
    "expected_purchased_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4017, 0.6451, 1.1112, 0.7266, 2.1968, 0.6525, 1.4064, 1.8073],\n",
       "        [1.1270, 0.4600, 0.6984, 1.0960, 2.3936, 1.1823, 1.4978, 1.9419]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A more direct way of calculating expected purchased vectors]\n",
    "expected_purchased_vecs = t.stack((\n",
    "    emb_39091 + emb_72705 + emb_31948,\n",
    "    emb_65162 + emb_31528 + emb_39091\n",
    "))\n",
    "expected_purchased_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_embeddings_bag = t.nn.EmbeddingBag.from_pretrained(pre_trained_product_embs, mode=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4017, 0.6451, 1.1112, 0.7266, 2.1968, 0.6525, 1.4064, 1.8073],\n",
       "        [1.1270, 0.4600, 0.6984, 1.0960, 2.3936, 1.1823, 1.4978, 1.9419]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchased_vecs = purchase_embeddings_bag(purchased_batch)\n",
    "purchased_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_purchased_vecs.allclose(purchased_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uneven Tokens\n",
    "While in the above example each row has the same number of tokens, 3 in case of the purchased features, this is not always true. E.g., if my feature was product IDs of products purchased in the last month, this will vary for each user and there each row will have a different number of product IDs.  \n",
    "\n",
    "| Purchase History |\n",
    "| ---------------- |\n",
    "| uxm3 a12o 8u2x   |\n",
    "| eri8 wi3r        |\n",
    "| w29k             |\n",
    "\n",
    "To accomodate this use case, the `EmbeddingBag` layer also takes two flat integer tensors, the first one is a flattened list of all the tokens in the batch. The second is a list of indexes demarcating the example boundaries. Even though the documentation calls this \"offsets\", they mean offsets from the beginning of the list, so really...indexes ü§∑üèæ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "After replacing tokens with their indexes lets say we get the following column -\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\color{lightgreen}4445 & \\color{lightgreen}5576 & \\color{lightgreen}251 \\\\\n",
    "\\color{orange}8747 & \\color{orange}8236 & \\\\\n",
    "\\color{cyan}880\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After flattening this batch we will get -\n",
    "$$\n",
    "values = \\begin{bmatrix} \\color{lightgreen}4445 & \\color{lightgreen}5576 & \\color{lightgreen}251 & \\color{orange}8747 & \\color{orange}8236 & \\color{cyan}880\\end{bmatrix} \\\\\n",
    "offsets = \\begin{bmatrix} \\color{lightgreen}0 & \\color{orange} 3 & \\color{cyan} 5 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product 4445 =  tensor([0.8331, 0.7415, 0.4044, 0.3748, 0.8785, 0.6459, 0.0478, 0.6405])\n",
      "product 5576 =  tensor([0.5124, 0.3451, 0.7072, 0.7841, 0.6407, 0.1628, 0.7097, 0.6242])\n",
      "product 251 =  tensor([0.3455, 0.6196, 0.0983, 0.4827, 0.4779, 0.3397, 0.1890, 0.9614])\n",
      "product 8747 =  tensor([0.9107, 0.8435, 0.9811, 0.5535, 0.9616, 0.1972, 0.9600, 0.9077])\n",
      "product 8236 =  tensor([0.9844, 0.5585, 0.5481, 0.8700, 0.7604, 0.0689, 0.0379, 0.7440])\n",
      "product 880 =  tensor([0.2010, 0.2330, 0.0684, 0.6042, 0.3434, 0.3394, 0.6218, 0.7564])\n"
     ]
    }
   ],
   "source": [
    "emb_4445 = pre_trained_product_embs[4445]\n",
    "emb_5576 = pre_trained_product_embs[5576]\n",
    "emb_251 = pre_trained_product_embs[251]\n",
    "emb_8747 = pre_trained_product_embs[8747]\n",
    "emb_8236 = pre_trained_product_embs[8236]\n",
    "emb_880 = pre_trained_product_embs[880]\n",
    "\n",
    "print(\"product 4445 = \", emb_4445)\n",
    "print(\"product 5576 = \", emb_5576)\n",
    "print(\"product 251 = \", emb_251)\n",
    "print(\"product 8747 = \", emb_8747)\n",
    "print(\"product 8236 = \", emb_8236)\n",
    "print(\"product 880 = \", emb_880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6910, 1.7062, 1.2100, 1.6415, 1.9971, 1.1485, 0.9465, 2.2262],\n",
       "        [1.8951, 1.4019, 1.5293, 1.4235, 1.7220, 0.2661, 0.9979, 1.6517],\n",
       "        [0.2010, 0.2330, 0.0684, 0.6042, 0.3434, 0.3394, 0.6218, 0.7564]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_purchased_vecs = t.stack((\n",
    "    emb_4445 + emb_5576 + emb_251,\n",
    "    emb_8747 + emb_8236,\n",
    "    emb_880\n",
    "))\n",
    "\n",
    "expected_purchased_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6910, 1.7062, 1.2100, 1.6415, 1.9971, 1.1485, 0.9465, 2.2262],\n",
       "        [1.8951, 1.4019, 1.5293, 1.4235, 1.7220, 0.2661, 0.9979, 1.6517],\n",
       "        [0.2010, 0.2330, 0.0684, 0.6042, 0.3434, 0.3394, 0.6218, 0.7564]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchased_batch_flat = t.tensor([4445, 5576, 251, 8747, 8236, 880])\n",
    "purchased_batch_offsets = t.tensor([0, 3, 5])\n",
    "purchased_vecs = purchase_embeddings_bag(purchased_batch_flat, purchased_batch_offsets)\n",
    "purchased_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.allclose(expected_purchased_vecs, purchased_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "248aaaef2ce54d580c63d0621ea4262f68d22105863a800468921850ea527b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
