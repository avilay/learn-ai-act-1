{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](../index.html) > [PyTorch](index.html) > Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Loss\n",
    "[Doc](https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.L1Loss)\n",
    "Given two vectors $\\hat{\\mathbf{y}}$ and $\\mathbf{y}$, this function calculates the loss as $\\vert \\hat{\\mathbf{y}} - \\mathbf{y} \\vert$\n",
    "\n",
    "I can specify if I want to average the losses or sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss(reduction='mean')\n",
    "y_hat = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([1.2, 2.2, 3.2])\n",
    "loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss\n",
    "[Doc](https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.MSELoss). Given two vectors $\\hat{\\mathbf{y}}$ and $\\mathbf{y}$, this function calcualtes the loss as $(\\hat{\\mathbf{y}} - \\mathbf{y})^2$.\n",
    "\n",
    "I can specify if want to average the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0400)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss(reduction='elementwise_mean')\n",
    "y_hat = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([1.2, 2.2, 3.2])\n",
    "loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLLoss\n",
    "[Doc](). The Negative Log Likelihood Loss. The inputs to the loss function are -\n",
    "\n",
    "  * A matrix of the log probabilities of size m x c where each element is $l_k^{(i)} = log(p_k^{(i)})$ \n",
    "  * The target class\n",
    "  \n",
    "The target class is **not** encoded as a one-hot vector. It is supposed to be a vector of size m where each element $y^{(i)}$ is an integer from 0 to c-1. This function does not care how the probabilities (and therefore the log probabilities) were calculated. These would be model dependant.\n",
    "\n",
    "This function will then do the following for each instance (dropping the superscript (i) for clarity)) -\n",
    "\n",
    "  1. Create a one-hot-encoded vector from the target value.\n",
    "  2. Calculate the negative log-likelihood $-\\mathcal L = -\\sum_{k=1}^c y_k l_k$. \n",
    "\n",
    "And finally, take the average across the entire mini-batch.\n",
    "  \n",
    "The first two steps are simply selecting the log probability of the target class for each example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minibatch size m = 2\n",
    "# Number of classes c = 3\n",
    "l = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "y = torch.tensor([0, 1])\n",
    "\n",
    "# This will select 1 from the first row of l and 5 from its second row, negate the values,\n",
    "# and return the average of these two numbers = -3\n",
    "loss_fn = nn.NLLLoss(reduction='elementwise_mean')\n",
    "loss_fn(l, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "[Doc](https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.CrossEntropyLoss). The inputs to the loss function are -\n",
    "\n",
    "  * A matrix of the logits of size m x c where each element is $h_k^{(i)}$\n",
    "  * The target class\n",
    "  \n",
    "The target class is **not** encoded as a one-hot vector. It is supposed to be a vector of size M where each element $y^{(i)}$ is an integer from 0 to c-1. \n",
    "\n",
    "This function will then do the following for each instance (dropping the superscript (i) for clarity) -\n",
    "\n",
    "  1. Create a one-hot encoded vector based on the target class.\n",
    "  2. Calculate the softmax probabilities for each class.\n",
    "$$\n",
    "p_k = \\frac {e^{h_k}}{\\sum_{j=1}^c e^{h_j}}\n",
    "$$\n",
    "  3. Calculate the negative log likelihood $-\\mathcal L = -\\sum_{k=1}^c y_k log(p_k)$.\n",
    "  \n",
    "And finally take the average across the entire mini-batch.\n",
    "\n",
    "This function is a bit different from other loss functions in which it will calculate the softmax probabilities. Most other loss functions just need the probabilities, they don't really care how these probabilities were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9076)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minibatch size m = 2\n",
    "# Number of classes k = 3\n",
    "h = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "y = torch.tensor([0, 1])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='elementwise_mean')\n",
    "loss_fn(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "tensor([[-2.4076, -1.4076, -0.4076],\n",
      "        [-2.4076, -1.4076, -0.4076]])\n",
      "tensor(-2.4076)\n",
      "tensor(-1.4076)\n",
      "tensor(1.9076)\n"
     ]
    }
   ],
   "source": [
    "# Lets do this by hand\n",
    "p = F.softmax(h, dim=1)\n",
    "l = torch.log(p)\n",
    "print(p)\n",
    "print(l)\n",
    "\n",
    "# Log likelihood of the first row is the 0th element because y[0] = 0\n",
    "print(l[0, 0])\n",
    "\n",
    "# Log likelihood of the second row is the 1st element because y[1] = 1\n",
    "print(l[1, 1])\n",
    "\n",
    "# The final loss is the average of the negative log likelihoods\n",
    "loss = ((-l[0, 0]) + (-l[1, 1])) / 2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy\n",
    "Can be used for both binary classification and multi-class classification. There are two modules for this, the `BCELoss` and `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with `BCELoss`\n",
    "\n",
    "[Doc](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss). The inputs to the loss function are -\n",
    "\n",
    "  * A vector of probabilities of length m where each element is $p^{(i)}$\n",
    "  * The target class\n",
    "\n",
    "Mathematically speaking it makese sense to have the target class be either $0$ or $1$, but this function does not really care whether this is true or not. The function does not care how the probabilities were calculated. That is model dependant.\n",
    "\n",
    "This function will calculate the negative log likelihood of each instance as $-\\mathcal L^{(i)} = -\\left[y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)}) \\right]$ and then aggregate across the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5108)\n",
      "0.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1., 0., 1.])\n",
    "p = torch.tensor([0.9, 0.7, 0.8])\n",
    "loss_fn = nn.BCELoss(reduction='elementwise_mean')\n",
    "loss = loss_fn(p, y)\n",
    "print(loss)\n",
    "\n",
    "# By hand\n",
    "l = [None, None, None]\n",
    "l[0] = np.log(0.9)\n",
    "l[1] = np.log(0.3)\n",
    "l[2] = np.log(0.8)\n",
    "loss = - sum(l)/3\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label Classification with `BCELoss`\n",
    "The inputs to the loss function are -\n",
    "\n",
    "  * A matrix of probabilities of size m x c, where each element $p_k^{(i)}$ is the probability of the $i$th instance having the $k$th label.\n",
    "  * A matrix of target classes of size m x c, where each element $y_k^{(i)}$ is $1$ if the $i$th instance has the $k$th label and $0$ otherwise.\n",
    " \n",
    "Mathematically speaking it makese sense for the output matrix to be comprised only of $0$ and $1$s, but this function does not really care about that. Further it does not care how the probabilities were calculated. That is model dependant.\n",
    "\n",
    "This function will calculate the negative log likelihood of each instance as (dropping the superscript i)-\n",
    "\n",
    "$$\n",
    "- \\mathcal L = - \\frac1c \\sum_{k=1}^c \\left[y_k\\;log(p_k) + (1-y_k)\\;log(1-p_k) \\right]\n",
    "$$\n",
    "\n",
    "And finally take the average across the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4149)\n",
      "0.41493159961539705\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([[1., 1., 0.],\n",
    "                  [0., 1., 1.]])\n",
    "p = torch.tensor([[0.9, 0.8, 0.2],\n",
    "                  [0.7, 0.8, 0.6]])\n",
    "loss_fn = nn.BCELoss(reduction='elementwise_mean')\n",
    "loss = loss_fn(p, y)\n",
    "print(loss)\n",
    "\n",
    "# By hand\n",
    "l = [None, None]\n",
    "l[0] = (np.log(0.9) + np.log(0.8) + np.log(0.8))/3\n",
    "l[1] = (np.log(0.3) + np.log(0.8) + np.log(0.6))/3\n",
    "loss = - sum(l) / 2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with `BCEWithLogitsLoss`\n",
    "\n",
    "[Doc](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Inputs to the loss function are - \n",
    "  * A vector of affines with length $m$ where each element is $h^{(i)}$.\n",
    "  * The target class\n",
    "\n",
    "This function applies the sigmoid function to the affines and then calculates the familiar BCE loss as follows:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-h}} \\\\\n",
    "- \\mathcal L = - \\left[y\\;log p + (1-y)\\;log (1-p) \\right]\n",
    "$$\n",
    "\n",
    "And then aggregate across the minibatch. The loss function can be initialized to use a weightage for the positive examples. Lets say it is set to $3$, then each positive example behaves as if there were $3$ positive examples. This is not shown in the demo below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5101)\n",
      "tensor([0.9011, 0.7006, 0.8022])\n",
      "tensor(0.5101)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1., 0., 1.])\n",
    "h = torch.Tensor([2.21, 0.85, 1.4])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(h, y)\n",
    "print(loss)\n",
    "\n",
    "# By hand\n",
    "p = torch.sigmoid(h)\n",
    "print(p)\n",
    "loss = torch.nn.BCELoss()(p, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "248aaaef2ce54d580c63d0621ea4262f68d22105863a800468921850ea527b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
