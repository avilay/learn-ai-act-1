{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from data import *\n",
    "from train import *\n",
    "from tune import *\n",
    "from haikunator import Haikunator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = Path.home() / \"mldata\"\n",
    "RUNROOT = Path.home() / \"mlruns\" / \"imdb\"\n",
    "t.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, _ = get_vocab(DATAROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "Lets learn the embeddings along with the rest of the NN. The NN architecture is very simple, a couple of fully connected layers with ReLU activation. After hyperparameter tuning the best hyper parameters are:\n",
    "  * max_epochs: 9\n",
    "  * learning_rate: 0.00158223651599205\n",
    "  * max_seq_len: 100\n",
    "  * batch_size': 8\n",
    "  * embedding_dim: 25\n",
    "\n",
    "Giving an accuracy of 73.79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Simple\n",
    "\n",
    "simple = Simple(vocab_size=len(vocab), max_seq_len=100, embedding_dim=25)\n",
    "print(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = pl.loggers.CSVLogger(save_dir=RUNROOT.parent, name=RUNROOT.name) # type: ignore\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=RUNROOT,\n",
    "    max_epochs=9,\n",
    "    logger=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    enable_progress_bar=True,\n",
    "    # MPS does not work because aten::unique op is not implemented\n",
    "    # accelerator='mps', \n",
    "    # devices=1\n",
    "    # gpus=1 if t.cuda.is_available() else None\n",
    "    accelerator=\"gpu\" if t.cuda.is_available() else \"cpu\",\n",
    "    devices=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ImdbDataModule(\n",
    "    dataroot=DATAROOT,\n",
    "    vocab=vocab,\n",
    "    max_seq_len=100,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "model = LitImdb(\n",
    "    model_factory=Simple,  # type: ignore\n",
    "    vocab_size=len(vocab),\n",
    "    max_seq_len=100,\n",
    "    embedding_dim=25,\n",
    "    learning_rate=0.00158223651599205\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.validate(datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Params: {'max_epochs': 9, 'max_seq_len': 500, 'batch_size': 64, 'embedding_dim': 200, 'learning_rate': 0.0009252345757324095}\n",
    "Values: ({'accuracy': 0.6941666603088379}, {'accuracy': {'accuracy': 0.0}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
