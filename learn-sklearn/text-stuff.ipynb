{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c40d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb139688",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fee3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b611f",
   "metadata": {},
   "source": [
    "This vectorizer will tokenize a string into individual words. A word is defined as having at least 2 letters. Single letter words are dropped. As can be seen in the example below, the word *a* is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d92cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = vectorizer.build_analyzer()\n",
    "tokenize(\"This is a text document to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bf16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document\",\n",
    "    \"This is the second second document\",\n",
    "    \"And the third one\",\n",
    "    \"Is this the first document\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2b05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = vectorizer.fit_transform(corpus)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595fe6bd",
   "metadata": {},
   "source": [
    "Each row of $D$ is a document to token mapping and each column is a token to document mapping, i.e, row $i$ has all the tokens that appeared in document $i$ and column $j$ has all the documents that contain the word $j$. In other words, each cell $d_{i,j}$ is the frequency of word $j$ in document $i$.\n",
    "\n",
    "#### Note\n",
    "It is tempting to draw parallels betweeen each row of $D$ and one-hot vectors. But remember that one-hot vectors represent a **single** word, here each row of $D$ represents the entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a9a138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = D.toarray()\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be618e",
   "metadata": {},
   "source": [
    "Given that the indices represent individual tokens, the `CountVectorizer` object has methods to get the tokens <--> idx mapping as shown below. \n",
    "\n",
    "`get_feature_names` method gives the list of tokens in their index order, i.e, the token with idx 0 will be the first element of this output and so on. It is really an `idx_to_token` mapping.\n",
    "\n",
    "`vocabulary_` method outputs an actual map this time, with the token as the key and its idx as the value. This is the `token_to_idx` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d312c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fa86c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] = and\n",
      "[1] = document\n",
      "[2] = first\n",
      "[3] = is\n",
      "[4] = one\n",
      "[5] = second\n",
      "[6] = the\n",
      "[7] = third\n",
      "[8] = this\n"
     ]
    }
   ],
   "source": [
    "idx_to_token = vectorizer.get_feature_names()\n",
    "for idx, token in enumerate(idx_to_token):\n",
    "    print(f\"[{idx}] = {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c414639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f1e8ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[this] = 8\n",
      "[is] = 3\n",
      "[the] = 6\n",
      "[first] = 2\n",
      "[document] = 1\n",
      "[second] = 5\n",
      "[and] = 0\n",
      "[third] = 7\n",
      "[one] = 4\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = vectorizer.vocabulary_\n",
    "for token, idx in token_to_idx.items():\n",
    "    print(f\"[{token}] = {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb10aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(docvec):\n",
    "    \"\"\"\n",
    "    Takes a vectorized document, i.e., a row in X, and outputs its contents in the\n",
    "    form of a bag of words. A bag of words is a list with the tokens that appear in\n",
    "    the document in index order. Words that appear multiple times in the doc are\n",
    "    repeated in the output.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for idx, freq in enumerate(docvec):\n",
    "        if freq > 0:\n",
    "            tokens += [idx_to_token[idx]] * freq\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd8f7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words(D[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00b00f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'is', 'second', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words(D[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4c8da",
   "metadata": {},
   "source": [
    "The vectorizer's vocab and bag of words was built with the `fit_transform`. The `transform` method will take in **new** documents and create a vectorized document just like rows of $D$. It will drop any words that were not present in the original corpus. Think of these docs as queries querying the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a39595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdocs = vectorizer.transform([\n",
    "   \"This is a new document and a new example\",\n",
    "    \"And another document here\",\n",
    "    \"Something completely new\"\n",
    "]).toarray()\n",
    "newdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eedea7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 0 0 1] ['and', 'document', 'is', 'this']\n",
      "[1 1 0 0 0 0 0 0 0] ['and', 'document']\n",
      "[0 0 0 0 0 0 0 0 0] []\n"
     ]
    }
   ],
   "source": [
    "for docvec in newdocs:\n",
    "    print(docvec, bag_of_words(docvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bf2a7",
   "metadata": {},
   "source": [
    "The default `CountVectorizer` uses space to tokenize. But we an give it regex pattern to build ourselves a bi-gram vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793b76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r\"\\b\\w+\\b\", min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d5a907f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = bigram_vectorizer.build_analyzer()\n",
    "tokenize(\"Bi-grams are cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89902f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef59444",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87ae64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([[3, 0, 1],\n",
    "              [2, 0, 0],\n",
    "              [3, 0, 0],\n",
    "              [4, 0, 0],\n",
    "              [3, 2, 0],\n",
    "              [3, 0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eec1a",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "### Token Frequency\n",
    "Token (or term) frequency is the number of times token $j$ has appeared in document. It is just the $j_{th}$ elmenet of the $i_{th}$ document so we can just the read this value of the main corpus $D$ matrix: $d_{i, j}$.\n",
    "\n",
    "### Inverse Document Frequenecy\n",
    "Document frequency of the token is the number of documents that have the token $j$: $f_j = \\sum_i \\mathbb{1}_{d_{i, j} > 0}$. This is just the mathematical way of saying count the number of non-zero cells in the $j_{th}$ column. The inverse document frequency is just the ratio of the total number of documents to the document frequency, $\\frac{n}{f_j}$. Here $n$ is the total number of documents. The range of IDF is $[0, 1]$. Generally speaking a more popular token should have a higher IDF. On the other hand, very frequent tokens are less important for information retrieval because they don't provide any additional information about the document it appears in. For this reason, we usually take the log of IDF.\n",
    "\n",
    "### Calculating tf-idf for document $i$ and token $j$\n",
    "The first step in calculating the tf-idf of token $j$ in document $i$ is -\n",
    "$$\n",
    "x'_{i, j} = d_{i, j}  \\left( log \\frac{n}{f_j} + 1 \\right)\n",
    "$$\n",
    "\n",
    "After doing this for each token in the document, we then normalize the document's tf-idf vector.\n",
    "\n",
    "$$\n",
    "x_{i,j} = \\frac{x'_{i,j}}{\\left \\| \\mathbf x'_i \\right \\|} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b435ec4",
   "metadata": {},
   "source": [
    "Lets calculate the tf-idf for all the tokens in the first document.\n",
    "$$\n",
    "x'_{0,0} = d_{0,0} \\left( log \\frac{n}{f_0} + 1 \\right) \\\\\n",
    "x'_{0,1} = d_{0,1} \\left( log \\frac{n}{f_1} + 1 \\right) \\\\\n",
    "x'_{0,2} = d_{0,2} \\left( log \\frac{n}{f_2} + 1 \\right) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd64fcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = D[0,:]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae7cd815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0 = np.sum(D[:,0] > 0)\n",
    "f1 = np.sum(D[:,1] > 0)\n",
    "f2 = np.sum(D[:,2] > 0)\n",
    "f = np.array([f0, f1, f2])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45302744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = D.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9263550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.   , 0.   , 2.099])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0_ = d * (np.log(n/f)  + 1)\n",
    "x0_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebe616",
   "metadata": {},
   "source": [
    "The norm of this vector is $\\left \\| \\mathbf x'_0 \\right \\| = \\sqrt{{x'}_{0,0}^2 + x_{0,1}^2 + x_{0,2}^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd5e2e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6611710610334507"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0_norm = np.linalg.norm(x0_)\n",
    "x0_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe4fd502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6611710610334507"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above function calculates the L2 norm\n",
    "np.sqrt(x0_[0]**2 + x0_[1]**2 + x0_[2]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e929d",
   "metadata": {},
   "source": [
    "The normalized tf-idf vector for all the tokens in the first document are -\n",
    "$$\n",
    "x_0 = \\frac{x'_0}{\\left \\| \\mathbf x'_0 \\right \\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b264fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.819, 0.   , 0.573])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = x0_ / x0_norm\n",
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b5270",
   "metadata": {},
   "source": [
    "`sklearn` has a class that can do all of this in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "002fc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feeeaaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.819, 0.   , 0.573],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.473, 0.881, 0.   ],\n",
       "       [0.581, 0.   , 0.814]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xformer = TfidfTransformer(smooth_idf=False)\n",
    "X = xformer.fit_transform(D).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8d0f",
   "metadata": {},
   "source": [
    "So far we have seen the following pipeline - \n",
    "  1. Start with text corpus, typically a list of strings.\n",
    "  2. Convert this to a list of document vectors by passing them throug the `CountVectorizer`. We now have $D$.\n",
    "  3. Convert this to a matrix of tf-idfs by pasing $D$ through a `TfidfTransformer`.\n",
    "\n",
    "Lets run this pipeline on our original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfb2ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fee5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25bb0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document\",\n",
    "    \"This is the second second document\",\n",
    "    \"And the third one\",\n",
    "    \"Is this the first document\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ab274db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "\n",
      "TF-IDF\n",
      " [[0.    0.433 0.569 0.433 0.    0.    0.336 0.    0.433]\n",
      " [0.    0.24  0.    0.24  0.    0.89  0.186 0.    0.24 ]\n",
      " [0.561 0.    0.    0.    0.561 0.    0.235 0.561 0.   ]\n",
      " [0.    0.433 0.569 0.433 0.    0.    0.336 0.    0.433]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "D = count_vectorizer.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "X = tfidf_transformer.fit_transform(D).toarray()\n",
    "print(\"Documents\\n\", D.toarray())\n",
    "print(\"\\nTF-IDF\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91258033",
   "metadata": {},
   "source": [
    "`sklearn` has a convenience class `TfidfVectorizer` to do all this, we just pass it the text corpus and get the tf-idf matrix out. This has the usual methods that we find on `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e49a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcd86021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.433, 0.569, 0.433, 0.   , 0.   , 0.336, 0.   , 0.433],\n",
       "       [0.   , 0.24 , 0.   , 0.24 , 0.   , 0.89 , 0.186, 0.   , 0.24 ],\n",
       "       [0.561, 0.   , 0.   , 0.   , 0.561, 0.   , 0.235, 0.561, 0.   ],\n",
       "       [0.   , 0.433, 0.569, 0.433, 0.   , 0.   , 0.336, 0.   , 0.433]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "X = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f37c909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f03daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd6fe441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([\"This is a new document and a new example\"]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a73d72b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.731, 0.394, 0.   , 0.394, 0.   , 0.   , 0.   , 0.   , 0.394])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.transform([\"This is a new document and a new example\"]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5310757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
