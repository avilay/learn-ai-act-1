{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run autograd we have to use the `tf.GradientTape` as the context object. We can use a `tf.Tensor` as the root variable on which the rest of the compute graph is built, or we can use a `tf.Variable` object for that. If we use a plain old `tf.Tensor` then we have to tell the gradient tape that we want to perform gradient calculations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones(4)  # same as x = tf.constant(np.ones(4))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)  # we tell the gradient tape that x is the variable\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 10:00:26.017770 4766352832 deprecation.py:323] From /Users/avilay/venvs/ai/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=32, shape=(4,), dtype=float32, numpy=array([18., 18., 18., 18.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx = t.gradient(z, x)\n",
    "dz_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.ones(4))  # No need to call t.watch(x) because x is a Variable\n",
    "with tf.GradientTape() as t:\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(4,), dtype=float32, numpy=array([18., 18., 18., 18.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx = t.gradient(z, x)\n",
    "dz_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the graidents between any two variable nodes in the graph. e.g., we can calculate $\\frac{dz}{dy}$ or $\\frac{dy}{dx}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones(4)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(4,), dtype=float32, numpy=array([18., 18., 18., 18.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dy = t.gradient(z, y)\n",
    "dz_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=113, shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.ones(4)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2\n",
    "\n",
    "dy_dx = t.gradient(y, x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But once a gradient tape has been used, i.e., `t.gradient` has been called on it, it cannot be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones(4)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2\n",
    "    \n",
    "dy_dx = t.gradient(y, x)\n",
    "print(dy_dx)\n",
    "\n",
    "try:\n",
    "    dz_dx = t.gradient(z, x)\n",
    "except RuntimeError as re:\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always create a reusable gradient tape if needed. But the gradients are immutable, i.e., once I have calculated $\\frac{dy}{dx}$, calling it repeatedly is going to give me the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([18. 18. 18. 18.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones(4)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x + 2\n",
    "    y2 = y ** 2\n",
    "    z = 3 * y2\n",
    "    \n",
    "dy_dx = t.gradient(y, x)\n",
    "print(dy_dx)\n",
    "\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(dz_dx)\n",
    "\n",
    "dy_dx = t.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate second order derivatives we have to use two gradient tapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones(4)\n",
    "with tf.GradientTape() as t0:\n",
    "    t0.watch(x)\n",
    "    with tf.GradientTape() as t1:\n",
    "        t1.watch(x)\n",
    "        y = x * x * x\n",
    "    dy_dx = t1.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=210, shape=(4,), dtype=float32, numpy=array([6., 6., 6., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2y_dx2 = t0.gradient(dy_dx, x)\n",
    "d2y_dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=195, shape=(4,), dtype=float32, numpy=array([3., 3., 3., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function decorator for better performance. For development this does not seem to be needed. I'll look at this later when I get the stage where I am deploying my models in prod or prod-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def square(v):\n",
    "    return v*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.)\n",
    "with tf.GradientTape() as t:\n",
    "    y = square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=32, shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
