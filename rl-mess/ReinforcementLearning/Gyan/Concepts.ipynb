{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setting\n",
    "\n",
    "The agent wakes up and finds itself in state $S_0$. Here it will take an action $A_0$ which will land it in another state $S_1$ and give it a reward $R_1$ for taking the action. Now it will take action $A_1$, get a reward $R_2$ and land in state $S_2$ and so on. In general, when in state $S_t$, the agent will take action $A_t$ which will land it in state $S_{t+1}$ and generate a reward of $R_{t+1}$.\n",
    "\n",
    "![setting](images/rl_setting.png)\n",
    "\n",
    "One step for the agent is made up of <$S_t$, $A_t$, $R_{t+1}$, $S_{t+1}$>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Problem Formulation\n",
    "In general the agent cares about maximizing its long term cumulative or discounted reward. Mathematically this is the expected reward over the agent's lifetime or until it reaches a terminal state. At time step $t$ the total future reward is given by -\n",
    "\n",
    "$$\n",
    "G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "$$\n",
    "\n",
    "Where $\\gamma$ is the discount factor. If $\\gamma = 0$ then the agent cares only about the immediate reward $R_{t+1}$, if $\\gamma = 1$ it cares about all future rewards equally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "A problem setting where the next state only depends on the current state (and the action taken by the agent in the current state), but not on any previous states. Of course the reward is more long  term than that. A full MDP is characterised by - \n",
    "\n",
    "<$\\mathcal S$, $\\mathcal A$, $\\mathcal R$, $\\mathcal P$, $\\gamma$>\n",
    "\n",
    "Where - <br> \n",
    "$\\mathcal S$ is the state space <br>\n",
    "$\\mathcal A$ is the action space <br>\n",
    "$\\mathcal R$ is the reward function which at a minimum takes as input the current state and action and outputs the reward. In a more general form it can take as input the current state, action, the next state the agent landed on and output the reward. <br>\n",
    "$\\mathcal P$ is the transition probability function, i.e., it gives $P(S_{t+1}\\;\\vert\\;S_t, A_t)$ <br>\n",
    "$\\gamma$ is the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "#### Deterministic Policy\n",
    "A function that tells the agent what action to take when it is in a particular state. It is a mapping $\\pi: \\mathcal S \\rightarrow \\mathcal A$. For each state $s \\in \\mathcal S$ it yields the action $a \\in \\mathcal A$ that the agent will choose while in state $s$.  \n",
    "\n",
    "#### Stochastic Policy\n",
    "A function that tells the agent what is the probability with which it should take an action when in a particular state. It is a mapping $\\pi: \\mathcal S x \\mathcal A \\rightarrow [0, 1]$. For each state $s \\in \\mathcal S$ and action $a \\in \\mathcal A$ it yields the probability $\\pi(a \\vert s)$ that the agent chooses action $a$ while in state $s$. Instead of taking the state and action as input and outputing the probability of taking that action, the stochastic policy can also be viewed as a function that takes as input the state (just like the deterministic policy) and outputs the action distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "#### State-Value Functions\n",
    "Lets say the agent finds itself in some state $s \\in \\mathcal S$, it does not matter how it got there. But from here on now, it has vowed to follow policy $\\pi$, then the expected reward it gets from this point on, is the said to be the **value** of the state $s$ under that policy.  \n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb E_{\\pi} \\left[ G_t \\vert S_t = s \\right]\n",
    "$$\n",
    "\n",
    "#### Action-Value Functions\n",
    "Lets say the agent finds itself in some state $s \\in \\mathcal S$ and has for some reason committed to take action $a \\in \\mathcal A$. We don't know how it got there or what is compelling it to take that action. But the agent has vowed that once it gets to the next state, it will only follow policy $\\pi$. The expected reward it gets from this point on is said to be the **action-value** of $s, a$ under policy $\\pi$.\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb E \\left[G_t \\vert S_t=s, A_t=a\\right]\n",
    "$$\n",
    "\n",
    "#### Relationship between state-value and action-value\n",
    "Lets say I want to find the state-value $v_\\pi(s_0)$. Based on the given policy the agent can take action $a_1$ with probability $p_1 = \\pi(a_1\\;\\vert\\;s_0)$ and action $a_2$ with probability $p_2 = \\pi(a_2\\;\\vert\\;s_0)$. Further I know the action-values $q_\\pi(s_0, a_1)$ and $q_\\pi(s_0, a_2)$. Then it follows that the state-value will simply be the **expected** action values over all possible actions, i.e., $v_\\pi(s_0) = p_1\\;q_\\pi(s_0, a_1) + p_2\\;q_\\pi(s_0, a_2)$. More generally -\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal A} \\pi(a \\vert s)\\;q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "Now lets say we want to find the action-value $q_\\pi(s_0, a_1)$ under the policy $\\pi$. From the reward function we know that the agent will get an immideate reward $r = \\mathcal R(s_0, a_1)$. From the transition probabilities of our MDP, we know that after taking action $a_1$, the agent will land in state $s_1$ with probability $p_1 = \\mathcal P(s_1 \\vert s_0, a_1)$ or in state $s_2$ with probability $p_2 = \\mathcal P(s_2 \\vert s_0, a_1)$. Further we know the state-values $v_\\pi(s_1)$ and $v_\\pi(s_2)$. It follows that the action-value will simply be the **expected** state-value over all possible states, i.e., $q_\\pi(s_0, a_1) = r + p_1\\;v_\\pi(s_1) + p_2\\;v_\\pi(s_2)$. More generally -\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_\\pi(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "An optimal policy is one following which we get the best value in all states. It is guranteed to exist but may not be unique. Obviously, all optimal policies have the same value functions called the **optimal state-value function** $v_*$ and the **optimal action-value function** $q_*$. The optimal policy can be defined in terms of the optimal action-value function as follows -\n",
    "\n",
    "$$\n",
    "\\pi_*(s) = argmax_a\\;q_*(s, a)\n",
    "$$\n",
    "\n",
    "Note, the optimal policy is always deterministic, i.e., given a state, it will output THE action for the agent to take.\n",
    "\n",
    "#### Relationship between optimal state-value and optimal action-value\n",
    "Lets say we want to find out the optimal state-value of state $s_2$ and we know the optimal action-values of $a_5$ and $a_6$ and further that $q_*(s_2, a_5) > q_*(s_2, a_6)$. Because the optimal state-value is the best possible state-value, we will always choose action $a_5$ when in state $s_2$, this means that $v_*(s_2) = q_*(s_2, a_5)$. More generally -\n",
    "\n",
    "$$\n",
    "v_*(s) = max_{a \\in \\mathcal A}\\;q_*(s, a)\n",
    "$$\n",
    "\n",
    "Now lets say we want to find the optimal action-value $q_*(s_0, a_1)$. We have the reward function so we can calculate the immideate reward the agent will get when it takes action $a_1$, lets denote it by $r = \\mathcal R(s_0, a_1)$. Now based on the transition probabilities of our MDP, after taking action $a_1$ our agent can land in state $s_1$ with probability $p_1 = \\mathcal P(s_1 \\vert s_0, a_1)$ or in state $s_2$ with probability $p_2 = \\mathcal P(s_2 \\vert s_0, a_1)$. We know the optimal value of both possible states $v_*(s_1)$ and $v_*(s_2)$. Because we are not sure where the agent might end up, the optimal action-value will simply consider the optimal **expected** value of all possible next states. $q_*(s_0, a_1) = r + p_1\\;v_*(s_1) + p_2\\;v_*(s_2)$. More generally -\n",
    "\n",
    "$$\n",
    "q_*(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Forumlation\n",
    "#### Prediction\n",
    "Given a policy, our job is to find (or predict) the state-value function under that policy. This in and of itself is not a very interesting problem to solve. But it is used to solve the next problem of **control**.\n",
    "\n",
    "$$\n",
    "Prediction: Env, \\pi \\rightarrow v_\\pi(s)\n",
    "$$\n",
    "\n",
    "#### Control\n",
    "Find the optimal policy. This can be done either by finding the optimal state-values or optimal action-values.\n",
    "\n",
    "$$\n",
    "Control: Env \\rightarrow \\pi_*\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "We can solve either the Prediction or the Control problem in the following two types of environments.\n",
    "\n",
    "#### Planning\n",
    "We are given the full MDP, most importantly the transition probabilities $\\mathcal P$ and the reward function $\\mathcal R$. \n",
    "\n",
    "$$\n",
    "Prediction: MDP, \\pi \\rightarrow v_\\pi(s) \\\\\n",
    "Control: MDP \\rightarrow \\pi_* \\\\\n",
    "$$\n",
    "\n",
    "#### Reinforcement Learning (model-free)\n",
    "Like most real-world scenarios, in this environment we don't know the transition probabilities or the reward function. We can only observe the state the agent actually landed in and the actual reward it got.\n",
    "\n",
    "$$\n",
    "Prediction: Incomplete MDP, \\pi \\rightarrow v_\\pi(s) \\\\\n",
    "Control: Incomplete MDP \\rightarrow \\pi_* \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equations\n",
    "#### Expectation\n",
    "This set of equations deal with values under some given policy $\\pi$ (not neccessarily the optimal policy). They are usually expressed as expectations, hence the name.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(s) &= \\mathbb E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\;\\vert\\; S_t = s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_\\pi(s') \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s, a) &= \\mathbb E_\\pi \\left[ R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\;\\vert\\; S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a \\sum_{a' \\in \\mathcal A} \\pi(a '\\vert s') q_\\pi(s', a') \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Optimality\n",
    "This set of equations deal with the optimal values.\n",
    "\n",
    "$$\n",
    "v_*(s) = max_{a \\in \\mathcal A} \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_*(s') \\right)\n",
    "$$\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "$$\n",
    "q_*(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a \\; max_{a' \\in \\mathcal A} \\; q_*(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation Summary\n",
    "\n",
    "### Definition Equaltions\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb E_{\\pi} \\left[ G_t \\vert S_t = s \\right] \\\\\n",
    "q_{\\pi}(s, a) = \\mathbb E \\left[G_t \\vert S_t=s, A_t=a\\right] \\\\\n",
    "\\pi_*(s) = argmax_a\\;q_*(s, a)\n",
    "$$\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Recursive Equations\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(s) &= \\mathbb E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\;\\vert\\; S_t = s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_\\pi(s') \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s, a) &= \\mathbb E_\\pi \\left[ R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\;\\vert\\; S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a \\sum_{a' \\in \\mathcal A} \\pi(a '\\vert s') q_\\pi(s', a') \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "v_*(s) = max_{a \\in \\mathcal A} \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_*(s') \\right) \\\\\n",
    "q_*(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a \\; max_{a' \\in \\mathcal A} \\; q_*(s', a')\n",
    "$$\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Relationships\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal A} \\pi(a \\vert s)\\;q_\\pi(s, a) \\\\\n",
    "q_\\pi(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_\\pi(s')\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "v_*(s) = max_{a \\in \\mathcal A}\\;q_*(s, a) \\\\\n",
    "q_*(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
