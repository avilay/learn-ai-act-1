{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Free Control\n",
    "The environment is an MDP but the agent does not know the reward function or the transition probabilities. The problem is to come up with an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLIE Monte-Carlo\n",
    "Start with the concept of generalized policy iteration.\n",
    "  1. Start with a random policy.\n",
    "  2. Evalute the current policy.\n",
    "  3. Improve the policy by acting greedily on the current values.\n",
    "\n",
    "There are two problems with this:\n",
    "  1. In order to generate the greedy policy from the state-values we need to know the transition probabilities. But in the model-free world we don't know that.\n",
    "The solution is to use action-values (colloquially called q-values) instead of state-values. Because generating the greedy policy from q-values is pretty straight forward.\n",
    "$$\n",
    "\\pi'(s) = argmax_{a \\in \\mathcal A}\\; q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "  2. The greedy policy will keep looking at certain greedy states. It worked when doing model-based control because we were averaging over all possible actions. In MC we only follow one action.\n",
    "The solution is to use a GLIE (Greedy in the Limit of Infinte Exploration) policy which has two properties:\n",
    "  (i) All state-action pairs are explored infinitely many times\n",
    "  (ii) The policy finally converges to a greedy policy\n",
    "Using an ε-greedy policy where $\\epsilon = \\frac1k$ so that it finally reduces to 0 thereby giving a fully greedy policy.\n",
    "\n",
    "A final improvement to the generalized policy iteration is to apply the idea from incremental MC prediction. Instead of generating a large number of episodes to do the full policy evaluation, and then updating the policy, we can start updating the policy after just one episode.\n",
    "\n",
    "The GLIE MC based policy iteration looks like -\n",
    "  1. Start with a random policy.\n",
    "  2. Generate a single episode under this policy.\n",
    "  3. Update the q-values by calculating the full reward for each state-action pair at the end of the episode ($G_t$).\n",
    "  4. Generate a new ε-greedy policy from the current Q-values setting $\\epsilon = \\frac1k$.\n",
    "  5. Repeat steps 2, 3, and 4 a lot of times.\n",
    "\n",
    "The fully GLIE MC Control algorithm is given by -\n",
    "![GLIE MC Control](../images/first-visit-alpha-glie-mc-ctrl-algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
