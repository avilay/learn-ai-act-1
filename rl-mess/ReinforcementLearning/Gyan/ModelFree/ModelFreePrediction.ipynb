{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Prediction\n",
    "This notebook discusses only the Prediction problem, i.e., given an environment and a policy, predict the value function under that policy. The environment used here is model-free, i.e., we don't know the transition probabilities of the MDP, nor do we know the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo\n",
    "  1. Generate a large number of episodes under the current policy.\n",
    "  2. Across all episodes, each time a state is visited, calculate the total return till the end of the episode ($G_t$). Do this for first visit or last visit.\n",
    "  3. Average the values for each state to give the state-value under the given policy.\n",
    "\n",
    "There is no need for convergence because by law of large numbers, for a sufficiently large number of episodes, the average state-value will approach the actual state-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Monte-Carlo\n",
    "Instead of generating a large number of episodes and then calculating the state-values at the end, incrementally update the state-value estimates after each episode. \n",
    "  1. Generate a single episode under the given policy.\n",
    "  2. For each state $S_T$ with total reward $G_t$, update $V(S_t)$ as follows:\n",
    "  $$\n",
    "  N(S_t) \\leftarrow N(S_t) + 1 \\\\\n",
    "  V(S_t) \\leftarrow V(S_t) + \\frac1{N(S_t)} (G_t - V(S_t))\n",
    "  $$\n",
    " \n",
    "Dividing the \"error\" term by $N(S_t)$ gives the true mean, but in non-stationary problems where the policy is changing with time, it is useful to consider an exponentially weighted average instead of the true average in order to \"forget\" older values. The update then becomes -\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "$$\n",
    "Where $\\alpha$ is a constant fraction.\n",
    "\n",
    "QUESTION: Is this done for a large number of episodes or until convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference (TD)\n",
    "Instead of waiting for an entire episode to complete before updating the value estimates, update them after each step.\n",
    "  1. Randomly set the initial value estimtes.\n",
    "  2. Start generating an episode under the given policy.\n",
    "  3. At each step update the value -\n",
    "  $$\n",
    "  V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "  $$\n",
    "  4. When the end of the episode is reached, start generating a new one and keep repeating.\n",
    "\n",
    "QUESTION: Is this done for a large number of episodes or until convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
