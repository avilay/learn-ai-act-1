{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "This notebook discusses various ways to solve the prediction and control problems in the **Planning** environment, i.e., we know the full mechanics of the MDP including its transiton probabilities $\\mathcal P$ and reward functions $\\mathcal R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Given a fully specified MDP and a policy (not neccessarily the optimal policy), determine the value function.\n",
    "$$\n",
    "\\text{Prediction: Full MDP}, \\pi \\rightarrow v_\\pi\n",
    "$$\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "Iteratively apply the Bellman expectation equation until the values converge.\n",
    "\n",
    "$$\n",
    "v_{k+1}(s) \\leftarrow \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_k(s') \\right)\n",
    "$$\n",
    "<p>&nbsp;</p>\n",
    "When the Bellman equation is satisfied, we know that we have reached convergence and that the values we have are the true values under the given policy.\n",
    "\n",
    "$$\n",
    "v_{k}(s) = \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_k(s') \\right) \\\\\n",
    "\\text{or, } v_{k+1}(s) = v_k(s) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control\n",
    "Given a fully specified MDP, determine the optimal policy (and also the optimal value function).\n",
    "$$\n",
    "\\text{Control: Full MDP} \\rightarrow \\pi_*, v_*\n",
    "$$\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "  1. Generate a random policy $\\pi$\n",
    "  2. Evaluate the current policy $\\pi$ $\\rightarrow v_\\pi$, can use the iterative policy evaluation with the Bellman expectation equation, but in general any policy evaluation aglorithm can be used.\n",
    "  $$\n",
    "  v_{k+1}(s) \\leftarrow \\sum_{a \\in \\mathcal A} \\pi_k(a \\vert s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_k(s') \\right)\n",
    "  $$\n",
    "  \n",
    "  3. Improve the policy. In this algo it is done by acting greedily w.r.t $v_\\pi$, $\\pi' = greedy(v_\\pi)$. But in general any policy improvement algorithm can be used.\n",
    "  $$\n",
    "  \\pi_{k+1}(s) \\leftarrow argmax_{a \\in \\mathcal A} \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_k(s') \\right)\n",
    "  $$\n",
    "\n",
    "In practice it is not neccessary to evaluate the policy completely, even after a few iterations, the value function is such that it can generate a pretty decent greedy policy that can then be used for the next iteration.\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Value Iteration\n",
    "Iteratively apply the Bellman optimality equation. There is no need for any policy.\n",
    "\n",
    "$$\n",
    "v_{k+1}(s) \\leftarrow max_{a \\in \\mathcal A} \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P_{ss'}^a v_k(s') \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
