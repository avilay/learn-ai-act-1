{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Silver Lectures\n",
    "\n",
    "### Lecture 3 Planning by DP\n",
    "Planning prediction and control approaches, i.e., MDP dynamics are known, find the value of a policy, or the optimal value function and optimal policy.\n",
    "  * Policy evaluation\n",
    "  * Policy iteration\n",
    "  * Value iteration\n",
    "  \n",
    "#### Practical_RL/week2_value_based\n",
    "<p>&nbsp;</p>  \n",
    "\n",
    "### Lecture 4 Model Free Prediction\n",
    "Model Free means that the MDP dynamics are not known, have to find the value of a policy purely from experience.\n",
    "  * Monte-Carlo learning\n",
    "  * TD learning\n",
    "  * TD(Î») learning\n",
    "\n",
    "#### Practical_RL/week3_model_free\n",
    "<p>&nbsp;</p>  \n",
    "\n",
    "### Lecture 5 Model Free Control\n",
    "Still in the Model Free regime, i.e., MDP dynamics are not known, have to find the optimal value function or optimal policy from experience. This is the real Reinforcement Learning problem.\n",
    "  * On-policy learning\n",
    "    * GLIE Monte-Carlo Control\n",
    "    * SARSA\n",
    "  * Off-policy learning\n",
    "    * Q-Learning\n",
    "    \n",
    "#### Practical_RL/week3_model_free    \n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Lecture 6 Value Function Approximation\n",
    "Use of NNs and other function approximators to solve model free prediction and control for large scale RL.\n",
    "  * Monte-Carlo with Value Function Approx\n",
    "  * TD-Learning with Value Function Approx\n",
    "  * Action-Value Function Approx\n",
    "  * Gradient Q-Learning\n",
    "  * DQN\n",
    "  \n",
    "#### Practical_RL/week4_approx_rl\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Lecture 7 Policy Gradient\n",
    "  * Finite Difference Policy Gradient\n",
    "  * Monte-Carlo Policy Gradient\n",
    "  * Actor-Critic Policy Gradient\n",
    "  \n",
    "#### Practical_RL/week6_policy_based\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Lecture 8 Integrated Learning and Planning\n",
    "Learn the model from experience (a.k.a Model Based) and then using Planning to construct value functions or policy. \n",
    "  * Planning with a learnt Model\n",
    "  * Dyna-Q\n",
    "  * MCTS\n",
    "  * TD-Search\n",
    "  * Dyna2\n",
    "\n",
    "#### Practical_RL/week2_value_based\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Lecture 9 Exploration and Exploitation\n",
    "  * Multi-Armed Bandits\n",
    "  * Contextual Bandits\n",
    "  * UCB\n",
    "  * Thompson\n",
    "  * Bayesian RL\n",
    "  \n",
    "#### Practical_RL/week5_explore\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "### Lecture 10 Classic Games\n",
    "  * Game Theory\n",
    "  * Minimax Search\n",
    "  * Self-Play RL\n",
    "  * Combining RL and Minimax Search\n",
    "  * RL in Imperfect-Information Games\n",
    "  \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
