from abc import ABC, abstractmethod
from typing import Tuple
from uuid import UUID

from .types import State, Action

"""
Class representing an environment that is goverened by a Markov Decision Process. It is conceivable
that an environment can have multiple agents.
"""

class Env(ABC):
    @abstractmethod
    def enter(self, agent_id: UUID) -> State:
        """
        The concrete implementation should set the agent to its starting state and return that state.
        As with curr_state, the starting state is dynamic, i.e., if curr_state is called after some time
        it is possible that it will not match the start state that is returned by this method. If this
        agent has already entered the environment, this method will raise DuplicateAgentError.

        Args:
            agent_id: The ID of the agent who is being initialized.

        Returns:
            State object representing the start state of the agent.

        Raises:
            DuplicateAgentError if agent_id has already entered the env.
        """
        raise NotImplementedError()

    @abstractmethod
    def curr_state(self, agent_id: UUID) -> State:
        """
        The current state that the agent is in. This can be dynamic and can change from one call to
        another. If the agent is not in the environment this will raise an UnknownAgentError.

        Args:
            agent_id: The ID of the agent whose state is being queried.

        Returns:
            The state object as visible to the specified agent.

        Raises:
            UnknownAgentError if the agent_id is not recognized.
        """
        raise NotImplementedError()

    @abstractmethod
    def move(self, agent_id: UUID, action: Action) -> Tuple[float, State]:
        """
        Move the specified agent to the next state based on the specified action. It returns with the
        reward generated by this step and the next state the agent landed in. However, the next state
        can be dynamic, i.e., if the agent queries for its current state after some time, it is possible
        that it will not match the next state that was returned by this method. If this agent never
        entered the system, this method will raise an UnknownAgentError.

        The next state is sampled from the underlying MDP -
        s_ = sample P(s_|s,a)

        The reward is also calculated by the underlying MDP -
        r = MDP.reward(s, a, s_)

        If an agent tries to move when its current state is terminal, an AgentTerminatedError
        should be raised.

        Args:
            agent_id: The ID of the agent who is making the move.
            action: The action that is being performed by this agent.

        Returns:
            A tuple whose first element is the generated reward and second element is the next state.

        Raises:
            AgentTerminatedError when agent's current state is terminal.
            UnknownAgentError when the agent_id is not recognized.
        """
        raise NotImplementedError()

    @abstractmethod
    def is_terminal(self, state: State) -> bool:
        """
        Checks if the specified state is a terminal state.
        """
        raise NotImplementedError()
